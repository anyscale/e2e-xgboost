{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "612b6a05",
   "metadata": {},
   "source": [
    "# Distributed Training of an XGBoost Model on Anyscale\n",
    "\n",
    "\n",
    "<div align=\"left\">\n",
    "<a target=\"_blank\" href=\"https://console.anyscale.com/\"><img src=\"https://img.shields.io/badge/ðŸš€ Run_on-Anyscale-9hf\"></a>&nbsp;\n",
    "<a href=\"https://github.com/anyscale/e2e-xgboost\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&amp;message=View%20On%20GitHub&amp;color=586069&amp;logo=github&amp;labelColor=2f363d\"></a>&nbsp;\n",
    "</div>\n",
    "\n",
    "This tutorial demonstrates how to execute a distributed training workload, connecting the following heterogeneous components:\n",
    "- Preprocessing the dataset with Ray Data\n",
    "- Distributed training of an XGBoost model with Ray Train\n",
    "- Saving model artifacts to a model registry, such as MLflow\n",
    "\n",
    "**Note**: This tutorial does not cover model tuning. Refer to [Ray Tune](https://docs.ray.io/en/latest/tune/index.html) for experiment execution and hyperparameter tuning at any scale.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-xgboost/refs/heads/main/images/distributed_training.png\" width=800>\n",
    "\n",
    "\n",
    "Before you start, follow the instructions in README.md to install the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5493d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cba9f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable importing from dist_xgboost module\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ddcfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Ray Train v2. This will be the default in an upcoming release.\n",
    "os.environ[\"RAY_TRAIN_V2_ENABLED\"] = \"1\"\n",
    "# now it's safe to import from ray.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cfb416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "from dist_xgboost.constants import local_storage_path, preprocessor_path\n",
    "\n",
    "ray.init(runtime_env={\"env_vars\": {\"TRAIN_ENABLE_SHARE_CUDA_VISIBLE_DEVICES\": \"1\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f79e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make ray data less verbose\n",
    "ray.data.DataContext.get_current().enable_progress_bars = False\n",
    "ray.data.DataContext.get_current().print_on_execution_start = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad88db8",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "This tutorial uses the [\"Breast Cancer Wisconsin (Diagnostic)\"](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic) dataset, which contains features computed from digitized images of breast mass cell nuclei.\n",
    "\n",
    "The data will be split as follows:\n",
    "- 70% for training\n",
    "- 15% for validation\n",
    "- 15% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1036655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data import Dataset\n",
    "\n",
    "\n",
    "def prepare_data() -> tuple[Dataset, Dataset, Dataset]:\n",
    "    \"\"\"Load and split the dataset into train, validation, and test sets.\"\"\"\n",
    "    # Load the dataset from S3\n",
    "    dataset = ray.data.read_csv(\"s3://anonymous@air-example-data/breast_cancer.csv\")\n",
    "    seed = 42\n",
    "\n",
    "    # Split 70% for training\n",
    "    train_dataset, rest = dataset.train_test_split(test_size=0.3, shuffle=True, seed=seed)\n",
    "    # Split the remaining 70% into 15% validation and 15% testing\n",
    "    valid_dataset, test_dataset = rest.train_test_split(test_size=0.5, shuffle=True, seed=seed)\n",
    "    return train_dataset, valid_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b0f220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split the dataset\n",
    "train_dataset, valid_dataset, _test_dataset = prepare_data()\n",
    "train_dataset.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65de1dd",
   "metadata": {},
   "source": [
    "Looking at the output, you can see the dataset contains features characterizing cell nuclei in breast mass, such as radius, texture, concavity, and symmetry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e67eb1",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Notice that the features have different magnitudes and ranges. While tree-based models like XGBoost aren't as sensitive to this, feature scaling can still improve numerical stability in some cases.\n",
    "\n",
    "Ray Data offers built-in preprocessors that simplify common feature preprocessing tasks, especially for tabular data. These can be seamlessly integrated with Ray Datasets, allowing you to preprocess your data in a fault-tolerant and distributed way.\n",
    "\n",
    "This example uses Ray's built-in `StandardScaler` to zero-center and normalize the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7256185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.preprocessors import StandardScaler\n",
    "\n",
    "\n",
    "def train_preprocessor(train_dataset: ray.data.Dataset) -> StandardScaler:\n",
    "    # pick some dataset columns to scale\n",
    "    columns_to_scale = [c for c in train_dataset.columns() if c != \"target\"]\n",
    "\n",
    "    # Initialize the preprocessor\n",
    "    preprocessor = StandardScaler(columns=columns_to_scale)\n",
    "    # train the preprocessor on the training set\n",
    "    preprocessor.fit(train_dataset)\n",
    "\n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "preprocessor = train_preprocessor(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19daa596",
   "metadata": {},
   "source": [
    "After fitting the preprocessor, the next step is to save it to a file. Later, this artifact can be registered in MLflow, allowing for reuse in downstream pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2688e721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(preprocessor_path, \"wb\") as f:\n",
    "    pickle.dump(preprocessor, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291d3be6",
   "metadata": {},
   "source": [
    "### Checkpointing Configuration\n",
    "\n",
    "Checkpointing is a powerful feature that enables you to resume training from the last checkpoint in case of interruptions. This is particularly useful for long-running training sessions.\n",
    "\n",
    "[`XGBoostTrainer`](https://docs.ray.io/en/latest/train/api/doc/ray.train.xgboost.XGBoostTrainer.html) implements checkpointing out of the box. We just need to configure [`RayTrainReportCallback`](https://docs.ray.io/en/latest/train/api/doc/ray.train.xgboost.RayTrainReportCallback.html) to set the checkpointing frequency.\n",
    "\n",
    "> **Note**: Once you enable checkpointing, you can follow [this guide](https://docs.ray.io/en/latest/train/user-guides/fault-tolerance.html) to enable fault tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ff2165",
   "metadata": {},
   "source": [
    "Next, the datasets are transformed using the fitted preprocessor. It's important to note that the `transform()` operation is lazy; it's only applied to the data when required by the train workers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230223b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = preprocessor.transform(train_dataset)\n",
    "valid_dataset = preprocessor.transform(valid_dataset)\n",
    "train_dataset.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ab598f",
   "metadata": {},
   "source": [
    "Using `take()`, you can observe that the values have been transformed to be zero-centered and rescaled roughly between -1 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128cb831",
   "metadata": {},
   "source": [
    "> **Data Processing Note**:  \n",
    "> For more advanced data loading and preprocessing techniques, check out the [comprehensive guide](https://docs.ray.io/en/latest/train/user-guides/data-loading-preprocessing.html). Ray Data also supports performant joins, filters, aggregations, and other operations for more structured data processing your workloads may require."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b534fa",
   "metadata": {},
   "source": [
    "## Model Training with XGBoost\n",
    "\n",
    "Now we can configure the training run using [`RunConfig`](https://docs.ray.io/en/latest/train/api/doc/ray.train.RunConfig.html#ray.train.RunConfig). Note that for XGBoost, we will configure checkpointing using `RayTrainReportCallback` instead of a `CheckpointConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9787bb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train import Result, RunConfig, ScalingConfig\n",
    "\n",
    "run_config = RunConfig(\n",
    "    ## For multi-node clusters, configure storage that is accessible\n",
    "    ## across all worker nodes with `storage_path=\"s3://...\"`\n",
    "    storage_path=local_storage_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9887b8e",
   "metadata": {},
   "source": [
    "### Training with XGBoost\n",
    "\n",
    "The training parameters are passed as a dictionary, similar to the original [`xgboost.train()`](https://xgboost.readthedocs.io/en/stable/parameter.html) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b793d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "config = {\n",
    "    \"model_config\": {\"objective\": \"binary:logistic\", \"eval_metric\": [\"logloss\", \"error\"], \"max_depth\": 2, \"verbosity\": 3},\n",
    "    \"checkpoint_frequency\": 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c22533",
   "metadata": {},
   "source": [
    "### Checkpointing Configuration\n",
    "\n",
    "Checkpointing is a powerful feature that enables you to resume training from the last checkpoint in case of interruptions. This is particularly useful for long-running training sessions.\n",
    "\n",
    "[`XGBoostTrainer`](https://docs.ray.io/en/latest/train/api/doc/ray.train.xgboost.XGBoostTrainer.html) implements checkpointing out of the box. You just need to configure [`RayTrainReportCallback`](https://docs.ray.io/en/latest/train/api/doc/ray.train.xgboost.RayTrainReportCallback.html) to set the checkpointing frequency.\n",
    "\n",
    "> **Note**: Once you enable checkpointing, you can follow [this guide](https://docs.ray.io/en/latest/train/user-guides/fault-tolerance.html) to enable fault tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a173cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from ray.train.xgboost import RayTrainReportCallback, XGBoostTrainer\n",
    "\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "\n",
    "def train_fn_per_worker(config: dict):\n",
    "    \"\"\"Training function that runs on each worker.\n",
    "\n",
    "    This function:\n",
    "    1. Gets the dataset shard for this worker\n",
    "    2. Converts to pandas for XGBoost\n",
    "    3. Separates features and labels\n",
    "    4. Creates DMatrix objects\n",
    "    5. Trains the model using distributed communication\n",
    "    \"\"\"\n",
    "    # Get this worker's dataset shard\n",
    "    train_ds, val_ds = (\n",
    "        ray.train.get_dataset_shard(\"train\"),\n",
    "        ray.train.get_dataset_shard(\"validation\"),\n",
    "    )\n",
    "    if USE_GPU:\n",
    "        config[\"device\"] = f\"cuda:{ray.train.get_context().get_local_rank()}\"\n",
    "\n",
    "    # Materialize the data and convert to pandas\n",
    "    train_ds = train_ds.materialize().to_pandas()\n",
    "    val_ds = val_ds.materialize().to_pandas()\n",
    "\n",
    "    # Separate the labels from the features\n",
    "    train_X, train_y = train_ds.drop(\"target\", axis=1), train_ds[\"target\"]\n",
    "    eval_X, eval_y = val_ds.drop(\"target\", axis=1), val_ds[\"target\"]\n",
    "\n",
    "    # Convert the data into DMatrix format for XGBoost\n",
    "    dtrain = xgboost.DMatrix(train_X, label=train_y)\n",
    "    deval = xgboost.DMatrix(eval_X, label=eval_y)\n",
    "\n",
    "    # Do distributed data-parallel training\n",
    "    # Ray Train sets up the necessary coordinator processes and\n",
    "    # environment variables for workers to communicate with each other\n",
    "    _booster = xgboost.train(\n",
    "        config[\"model_config\"],\n",
    "        dtrain=dtrain,\n",
    "        evals=[(dtrain, \"train\"), (deval, \"validation\")],\n",
    "        num_boost_round=10,\n",
    "        # Handles metric logging and checkpointing\n",
    "        callbacks=[\n",
    "            RayTrainReportCallback(\n",
    "                frequency=config[\"checkpoint_frequency\"],\n",
    "                checkpoint_at_end=True,\n",
    "                metrics=config[\"model_config\"][\"eval_metric\"],\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "trainer = XGBoostTrainer(\n",
    "    train_fn_per_worker,\n",
    "    train_loop_config=config,\n",
    "    # Register the data subsets\n",
    "    datasets={\"train\": train_dataset, \"validation\": valid_dataset},\n",
    "    # see \"How to scale out training?\" for more details\n",
    "    scaling_config=ScalingConfig(\n",
    "        # Number of workers for data parallelism.\n",
    "        num_workers=NUM_WORKERS,\n",
    "        # Set to True to use GPU acceleration\n",
    "        use_gpu=USE_GPU,\n",
    "    ),\n",
    "    run_config=run_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b48b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # attempt to run xgboost with GPU without ray train  FIXME\n",
    "\n",
    "# train_ds = train_dataset.materialize().to_pandas()\n",
    "# val_ds = valid_dataset.materialize().to_pandas()\n",
    "\n",
    "# train_X, train_y = train_ds.drop(\"target\", axis=1), train_ds[\"target\"]\n",
    "# eval_X, eval_y = val_ds.drop(\"target\", axis=1), val_ds[\"target\"]\n",
    "\n",
    "# dtrain = xgboost.DMatrix(train_X, label=train_y)\n",
    "# deval = xgboost.DMatrix(eval_X, label=eval_y)\n",
    "\n",
    "\n",
    "# _booster = xgboost.train(\n",
    "#         config[\"model_config\"],\n",
    "#         dtrain=dtrain,\n",
    "#         evals=[(dtrain, \"train\"), (deval, \"validation\")],\n",
    "#         num_boost_round=10,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70c0a68",
   "metadata": {},
   "source": [
    "> **Ray Train Benefits**:\n",
    "> \n",
    "> - **Multi-node orchestration**: Automatically handles multi-node, multi-GPU setup without manual SSH or hostfile configurations\n",
    "> - **Built-in fault tolerance**: Supports automatic retry of failed workers and can continue from the last checkpoint\n",
    "> - **Flexible training strategies**: Supports various parallelism strategies beyond just data parallel training\n",
    "> - **Heterogeneous cluster support**: Define per-worker resource requirements and run on mixed hardware\n",
    "> \n",
    "> Ray Train integrates with popular frameworks like PyTorch, TensorFlow, XGBoost, and more. For enterprise needs, [RayTurbo Train](https://docs.anyscale.com/rayturbo/rayturbo-train) offers additional features like elastic training, advanced monitoring, and performance optimization.\n",
    ">\n",
    "> <img src=\"https://raw.githubusercontent.com/anyscale/e2e-xgboost/refs/heads/main/images/train_integrations.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fe32cf",
   "metadata": {},
   "source": [
    "Now it's time to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005f33bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result: Result = trainer.fit()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf06ba2",
   "metadata": {},
   "source": [
    "Observe that at the beginning of the training job, Ray started requesting GPU nodes. This happens automatically to satisfy the training job's requirement of four GPU workers.\n",
    "\n",
    "Ray Train returns a [`ray.train.Result`](https://docs.ray.io/en/latest/train/api/doc/ray.train.Result.html) object, which contains important properties such as metrics, checkpoint info, and error details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c13bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = result.metrics\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b18221b",
   "metadata": {},
   "source": [
    "Expected output (your values may differ):\n",
    "\n",
    "```python\n",
    "OrderedDict([('train-logloss', 0.05463397157248817),\n",
    "             ('train-error', 0.00506329113924051),\n",
    "             ('validation-logloss', 0.06741214815308066),\n",
    "             ('validation-error', 0.01176470588235294)])\n",
    "```\n",
    "\n",
    "As shown in the output, Ray Train logged metrics based on the values configured in `eval_metric` and `evals`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e15f51a",
   "metadata": {},
   "source": [
    "It's also possible to reconstruct the trained model directly from the checkpoint directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87892b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "booster = RayTrainReportCallback.get_model(result.checkpoint)\n",
    "booster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f0523a",
   "metadata": {},
   "source": [
    "## Model Registry\n",
    "\n",
    "Now that the model is trained, the next step is to save it to a model registry for future use. This tutorial uses MLflow for this purpose, storing the model artifacts in [Anyscale user storage](https://docs.anyscale.com/configuration/storage/#user-storage). It's worth noting that Ray also integrates with [other experiment trackers](https://docs.ray.io/en/latest/train/user-guides/experiment-tracking.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba23e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from dist_xgboost.constants import (\n",
    "    experiment_name,\n",
    "    model_fname,\n",
    "    model_registry,\n",
    "    preprocessor_fname,\n",
    ")\n",
    "\n",
    "\n",
    "def clean_up_old_runs():\n",
    "    # clean up old MLFlow runs\n",
    "    os.path.isdir(model_registry) and shutil.rmtree(model_registry)\n",
    "    # mlflow.delete_experiment(experiment_name)\n",
    "    os.makedirs(model_registry, exist_ok=True)\n",
    "\n",
    "\n",
    "def log_run_to_mlflow(model_config, result, preprocessor_path):\n",
    "    # create a model registry in our user storage\n",
    "    mlflow.set_tracking_uri(f\"file:{model_registry}\")\n",
    "\n",
    "    # create a new experiment and log metrics and artifacts\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    with mlflow.start_run(description=\"xgboost breast cancer classifier on all features\"):\n",
    "        mlflow.log_params(model_config)\n",
    "        mlflow.log_metrics(result.metrics)\n",
    "\n",
    "        # Selectively log just the preprocessor and model weights\n",
    "        with TemporaryDirectory() as tmp_dir:\n",
    "            shutil.copy(\n",
    "                os.path.join(result.checkpoint.path, model_fname),\n",
    "                os.path.join(tmp_dir, model_fname),\n",
    "            )\n",
    "            shutil.copy(\n",
    "                preprocessor_path,\n",
    "                os.path.join(tmp_dir, preprocessor_fname),\n",
    "            )\n",
    "\n",
    "            mlflow.log_artifacts(tmp_dir)\n",
    "\n",
    "\n",
    "clean_up_old_runs()\n",
    "log_run_to_mlflow(config[\"model_config\"], result, preprocessor_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340529d0",
   "metadata": {},
   "source": [
    "You can start the MLflow server to view the logged experiments:\n",
    "\n",
    "`mlflow server -h 0.0.0.0 -p 8080 --backend-store-uri {model_registry}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c529b2c",
   "metadata": {},
   "source": [
    "To view the dashboard, go to the **Overview tab** â†’ **Open Ports** â†’ `8080`.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-xgboost/refs/heads/main/images/mlflow.png\" width=685>\n",
    "\n",
    "You can also view the Ray Dashboard and Train workload dashboards:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-xgboost/refs/heads/main/images/train_metrics.png\" width=700>\n",
    "\n",
    "The best model can then be retrieved from the registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d394cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dist_xgboost.data import get_best_model_from_registry\n",
    "\n",
    "best_model, artifacts_dir = get_best_model_from_registry()\n",
    "artifacts_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f54394",
   "metadata": {},
   "source": [
    "### Production Deployment with Anyscale Jobs\n",
    "\n",
    "We can wrap our training workload as a production-grade [Anyscale Job](https://docs.anyscale.com/platform/jobs/) ([API ref](https://docs.anyscale.com/reference/job-api/)):\n",
    "For production scenarios, the training workload can be wrapped as an [Anyscale Job](https://docs.anyscale.com/platform/jobs/) ([API ref](https://docs.anyscale.com/reference/job-api/)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d78b5c0",
   "metadata": {
    "tags": [
     "remove-cell-ci"
    ]
   },
   "outputs": [],
   "source": [
    "from dist_xgboost.constants import root_dir\n",
    "\n",
    "os.environ[\"WORKING_DIR\"] = root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb286b7",
   "metadata": {
    "tags": [
     "remove-cell-ci"
    ]
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Production batch job\n",
    "anyscale job submit --name=train-xboost-breast-cancer-model \\\n",
    "  --containerfile=\"${WORKING_DIR}/containerfile\" \\\n",
    "  --working-dir=\"${WORKING_DIR}\" \\\n",
    "  --exclude=\"\" \\\n",
    "  --max-retries=0 \\\n",
    "  -- python dist_xgboost/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557b6050",
   "metadata": {},
   "source": [
    "> **Note**: \n",
    "> - This example uses a `containerfile` to define dependencies, but using a pre-built image is also an option.\n",
    "> - You can specify compute requirements as a [compute config](https://docs.anyscale.com/configuration/compute-configuration/) or inline in a [job config](https://docs.anyscale.com/reference/job-api#job-cli)\n",
    "> - Note that when launched from a workspace without specifying compute, the job defaults to using the workspace's compute configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda2a7b0",
   "metadata": {},
   "source": [
    "## Scaling Strategies\n",
    "\n",
    "One of the key advantages of Ray Train is its ability to effortlessly scale your training workloads. By adjusting the [`ScalingConfig`](https://docs.ray.io/en/latest/train/api/doc/ray.train.ScalingConfig.html), you can optimize resource utilization and reduce training time.\n",
    "\n",
    "### Scaling Examples\n",
    "\n",
    "**Multi-node CPU Example** (4 nodes with 8 CPUs each):\n",
    "\n",
    "```python\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=4,\n",
    "    resources_per_worker={\"CPU\": 8},\n",
    ")\n",
    "```\n",
    "\n",
    "**Single-node multi-GPU Example** (1 node with 8 CPUs and 4 GPUs):\n",
    "\n",
    "```python\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=4,\n",
    "    use_gpu=True,\n",
    ")\n",
    "```\n",
    "\n",
    "**Multi-node multi-GPU Example** (4 nodes with 8 CPUs and 4 GPUs each):\n",
    "\n",
    "```python\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=16,\n",
    "    use_gpu=True,\n",
    ")\n",
    "```\n",
    "\n",
    "> **Important**: Keep in mind that for multi-node clusters, specifying a shared storage location (like cloud storage or NFS) in the `run_config` is necessary. Using a local path will cause an error during checkpointing.\n",
    ">\n",
    "> ```python\n",
    "> trainer = XGBoostTrainer(\n",
    ">     ..., run_config=ray.train.RunConfig(storage_path=\"s3://...\")\n",
    "> )\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdab5180",
   "metadata": {},
   "source": [
    "### Worker Configuration Guidelines\n",
    "\n",
    "The optimal number of workers depends on your workload and cluster setup:\n",
    "\n",
    "- For **CPU-only training**, generally use one worker per node (XGBoost can leverage multiple CPUs with threading)\n",
    "- For **multi-GPU training**, use one worker per GPU\n",
    "- For **heterogeneous clusters**, consider the greatest common divisor of CPU counts\n",
    "\n",
    "### GPU Acceleration\n",
    "\n",
    "To use GPUs for training:\n",
    "\n",
    "1. Start one actor per GPU with `use_gpu=True`\n",
    "2. Set GPU-compatible parameters\n",
    "3. Divide CPUs evenly across actors on each machine\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "trainer = XGBoostTrainer(\n",
    "    scaling_config=ScalingConfig(\n",
    "        # Number of workers to use for data parallelism.\n",
    "        num_workers=2,\n",
    "        # Whether to use GPU acceleration.\n",
    "        use_gpu=True,\n",
    "    ),\n",
    "    params={\n",
    "        # XGBoost specific params\n",
    "        \"eval_metric\": [\"logloss\", \"error\"],\n",
    "    },\n",
    "    ...\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021456cc",
   "metadata": {},
   "source": [
    "For more advanced topics, explore:\n",
    "- [Ray Tune](https://docs.ray.io/en/latest/tune/index.html) for hyperparameter optimization\n",
    "- [Ray Serve](https://docs.ray.io/en/latest/serve/index.html) for model deployment\n",
    "- [Ray Data](https://docs.ray.io/en/latest/data/data.html) for more advanced data processing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
