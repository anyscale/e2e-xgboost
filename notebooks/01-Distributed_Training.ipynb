{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "612b6a05",
   "metadata": {},
   "source": [
    "# Distributed Training of an XGBoost Model on Anyscale\n",
    "\n",
    "\n",
    "<div align=\"left\">\n",
    "<a target=\"_blank\" href=\"https://console.anyscale.com/\"><img src=\"https://img.shields.io/badge/ðŸš€ Run_on-Anyscale-9hf\"></a>&nbsp;\n",
    "<a href=\"https://github.com/anyscale/e2e-xgboost\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&amp;message=View%20On%20GitHub&amp;color=586069&amp;logo=github&amp;labelColor=2f363d\"></a>&nbsp;\n",
    "</div>\n",
    "\n",
    "In this tutorial, we'll execute a distributed training workload that will connect the following heterogenous workloads:\n",
    "- Preprocessing the dataset with Ray Data\n",
    "- Distributed training of an XGBoost model with Ray Train\n",
    "- Saving model artifacts to a model registry (MLFlow)\n",
    "\n",
    "**Note**: We won't be tuning our model in this tutorial, but be sure to check out [Ray Tune](https://docs.ray.io/en/latest/tune/index.html) for experiment execution and hyperparameter tuning at any scale.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-xgboost/refs/heads/main/images/distributed_training.png\" width=800>\n",
    "\n",
    "\n",
    "Let's start by installing the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f5493d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cba9f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable importing from dist_xgboost module\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23ddcfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Ray Train v2. This will be the default in an upcoming release.\n",
    "os.environ[\"RAY_TRAIN_V2_ENABLED\"] = \"1\"\n",
    "# Now it's safe to import from ray.train\n",
    "\n",
    "# Enable uv on Ray\n",
    "# https://docs.ray.io/en/latest/ray-core/handling-dependencies.html#using-uv-for-package-management\n",
    "os.environ.pop(\"RAY_RUNTIME_ENV_HOOK\", None)\n",
    "\n",
    "import ray\n",
    "\n",
    "ray.init(runtime_env={\"py_executable\": \"uv run\", \"working_dir\": \".\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15cfb416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dist_xgboost.constants import local_storage_path, preprocessor_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05f79e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make ray data less verbose\n",
    "ray.data.DataContext.get_current().enable_progress_bars = False\n",
    "ray.data.DataContext.get_current().print_on_execution_start = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad88db8",
   "metadata": {},
   "source": [
    "## Dataset Preparataion\n",
    "\n",
    "For this example, we're using the [\"Breast Cancer Wisconsin (Diagnostic)\"](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic) dataset, which contains features computed from digitized images of breast mass cell nuclei.\n",
    "\n",
    "We'll split the data into:\n",
    "- 70% for training\n",
    "- 15% for validation\n",
    "- 15% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1036655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data import Dataset\n",
    "\n",
    "\n",
    "def prepare_data() -> tuple[Dataset, Dataset, Dataset]:\n",
    "    \"\"\"Load and split the dataset into train, validation, and test sets.\"\"\"\n",
    "    # Load the dataset from S3\n",
    "    dataset = ray.data.read_csv(\"s3://anonymous@air-example-data/breast_cancer.csv\")\n",
    "    seed = 42\n",
    "\n",
    "    # Split 70% for training\n",
    "    train_dataset, rest = dataset.train_test_split(test_size=0.3, shuffle=True, seed=seed)\n",
    "    # Split the remaining 70% into 15% validation and 15% testing\n",
    "    valid_dataset, test_dataset = rest.train_test_split(test_size=0.5, shuffle=True, seed=seed)\n",
    "    return train_dataset, valid_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b0f220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split the dataset\n",
    "train_dataset, valid_dataset, _test_dataset = prepare_data()\n",
    "train_dataset.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65de1dd",
   "metadata": {},
   "source": [
    "Looking at the output, we can see the dataset contains features characterizing cell nuclei in breast mass, such as radius, texture, perimeter, area, smoothness, compactness, concavity, symmetry, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e67eb1",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Notice that the features have different magnitudes and ranges. While tree-based models like XGBoost aren't as sensitive to this, feature scaling can still improve numerical stability in some cases.\n",
    "\n",
    "Ray Data offers built-in preprocessors that simplify common feature preprocessing tasks, especially for tabular data. These can be seamlessly integrated with Ray Datasets, allowing you to preprocess your data in a fault-tolerant and distributed way.\n",
    "\n",
    "In this example, we'll use Ray's built-in `StandardScaler` to zero-center and normalize the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7256185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.preprocessors import StandardScaler\n",
    "\n",
    "# Select all columns except the target for scaling\n",
    "columns_to_scale = [c for c in train_dataset.columns() if c != \"target\"]\n",
    "\n",
    "# Initialize the preprocessor\n",
    "preprocessor = StandardScaler(columns=columns_to_scale)\n",
    "# Fit the preprocessor on the training set only\n",
    "# (this prevents data leakage)\n",
    "preprocessor.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19daa596",
   "metadata": {},
   "source": [
    "Now that we've fit the preprocessor, we'll save it to a file. Later, we'll register this artifact in MLFlow so we can reuse it in downstream pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2688e721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(preprocessor_path, \"wb\") as f:\n",
    "    pickle.dump(preprocessor, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ff2165",
   "metadata": {},
   "source": [
    "Next, we'll transform our datasets using the fitted preprocessor. Note that the `transform()` operation is lazy - it won't be applied to the data until it's required by the train workers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230223b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = preprocessor.transform(train_dataset)\n",
    "valid_dataset = preprocessor.transform(valid_dataset)\n",
    "train_dataset.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ab598f",
   "metadata": {},
   "source": [
    "Using `take()`, we can see that the values are now zero-centered and rescaled to be roughly between -1 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128cb831",
   "metadata": {},
   "source": [
    "> **Data Processing Note**:  \n",
    "> For more advanced data loading and preprocessing techniques, check out the [comprehensive guide](https://docs.ray.io/en/latest/train/user-guides/data-loading-preprocessing.html). Ray Data also supports performant joins, filters, aggregations, and other operations for more structured data processing your workloads may require."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b534fa",
   "metadata": {},
   "source": [
    "## Model Training with XGBoost\n",
    "\n",
    "### Checkpointing Configuration\n",
    "\n",
    "Checkpointing is a powerful feature that enables you to resume training from the last checkpoint in case of interruptions. This is particularly useful for long-running training sessions.\n",
    "\n",
    "[`XGBoostTrainer`](https://docs.ray.io/en/latest/train/api/doc/ray.train.xgboost.XGBoostTrainer.html) implements checkpointing out of the box. We just need to configure [`CheckpointConfig`](https://docs.ray.io/en/latest/train/api/doc/ray.train.CheckpointConfig.html) to set the checkpointing frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9787bb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train import CheckpointConfig, Result, RunConfig, ScalingConfig\n",
    "\n",
    "# Configure checkpointing to save progress during training\n",
    "run_config = RunConfig(\n",
    "    checkpoint_config=CheckpointConfig(\n",
    "        # Checkpoint every 10 iterations\n",
    "        checkpoint_frequency=10,\n",
    "        # Only keep the latest checkpoint\n",
    "        num_to_keep=1,\n",
    "    ),\n",
    "    ## For multi-node clusters, configure storage that is accessible\n",
    "    ## across all worker nodes with `storage_path=\"s3://...\"`\n",
    "    storage_path=local_storage_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaee233",
   "metadata": {},
   "source": [
    "> **Note**: Once you enable checkpointing, you can follow [this guide](https://docs.ray.io/en/latest/train/user-guides/fault-tolerance.html) to enable fault tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9887b8e",
   "metadata": {},
   "source": [
    "### Training with XGBoost\n",
    "\n",
    "The training parameters are passed as a dictionary, similar to the original [`xgboost.train()`](https://xgboost.readthedocs.io/en/stable/parameter.html) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a173cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from ray.train.xgboost import RayTrainReportCallback, XGBoostTrainer\n",
    "\n",
    "\n",
    "def train_fn_per_worker(config: dict):\n",
    "    \"\"\"Training function that runs on each worker.\n",
    "\n",
    "    This function:\n",
    "    1. Gets the dataset shard for this worker\n",
    "    2. Converts to pandas for XGBoost\n",
    "    3. Separates features and labels\n",
    "    4. Creates DMatrix objects\n",
    "    5. Trains the model using distributed communication\n",
    "    \"\"\"\n",
    "    # Get this worker's dataset shard\n",
    "    train_ds, val_ds = (\n",
    "        ray.train.get_dataset_shard(\"train\"),\n",
    "        ray.train.get_dataset_shard(\"validation\"),\n",
    "    )\n",
    "\n",
    "    # Materialize the data and convert to pandas\n",
    "    train_ds = train_ds.materialize().to_pandas()\n",
    "    val_ds = val_ds.materialize().to_pandas()\n",
    "\n",
    "    # Separate the labels from the features\n",
    "    train_X, train_y = train_ds.drop(\"target\", axis=1), train_ds[\"target\"]\n",
    "    eval_X, eval_y = val_ds.drop(\"target\", axis=1), val_ds[\"target\"]\n",
    "\n",
    "    # Convert the data into DMatrix format for XGBoost\n",
    "    dtrain = xgboost.DMatrix(train_X, label=train_y)\n",
    "    deval = xgboost.DMatrix(eval_X, label=eval_y)\n",
    "\n",
    "    # Do distributed data-parallel training\n",
    "    # Ray Train sets up the necessary coordinator processes and\n",
    "    # environment variables for workers to communicate with each other\n",
    "    _booster = xgboost.train(\n",
    "        config[\"xgboost_params\"],\n",
    "        dtrain=dtrain,\n",
    "        evals=[(dtrain, \"train\"), (deval, \"validation\")],\n",
    "        num_boost_round=10,\n",
    "        # Handles metric logging and checkpointing\n",
    "        callbacks=[RayTrainReportCallback()],\n",
    "    )\n",
    "\n",
    "\n",
    "# Parameters for the XGBoost model\n",
    "model_config = {\n",
    "    \"xgboost_params\": {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": [\"logloss\", \"error\"],\n",
    "    }\n",
    "}\n",
    "\n",
    "trainer = XGBoostTrainer(\n",
    "    train_fn_per_worker,\n",
    "    train_loop_config=model_config,\n",
    "    # Register the data subsets\n",
    "    datasets={\"train\": train_dataset, \"validation\": valid_dataset},\n",
    "    # see \"How to scale out training?\" for more details\n",
    "    scaling_config=ScalingConfig(\n",
    "        # Number of workers for data parallelism.\n",
    "        num_workers=5,\n",
    "        # Set to True to use GPU acceleration\n",
    "        use_gpu=True,\n",
    "    ),\n",
    "    run_config=run_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70c0a68",
   "metadata": {},
   "source": [
    "> **Ray Train Benefits**:\n",
    "> \n",
    "> - **Multi-node orchestration**: Automatically handles multi-node, multi-GPU setup without manual SSH or hostfile configurations\n",
    "> - **Built-in fault tolerance**: Supports automatic retry of failed workers and can continue from the last checkpoint\n",
    "> - **Flexible training strategies**: Supports various parallelism strategies beyond just data parallel training\n",
    "> - **Heterogeneous cluster support**: Define per-worker resource requirements and run on mixed hardware\n",
    "> \n",
    "> Ray Train integrates with popular frameworks like PyTorch, TensorFlow, XGBoost, and more. For enterprise needs, [RayTurbo Train](https://docs.anyscale.com/rayturbo/rayturbo-train) offers additional features like elastic training, advanced monitoring, and performance optimization.\n",
    ">\n",
    "> <img src=\"https://raw.githubusercontent.com/anyscale/e2e-xgboost/refs/heads/main/images/train_integrations.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fe32cf",
   "metadata": {},
   "source": [
    "Now let's train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005f33bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result: Result = trainer.fit()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf06ba2",
   "metadata": {},
   "source": [
    "Ray Train returns a [`ray.train.Result`](https://docs.ray.io/en/latest/train/api/doc/ray.train.Result.html) object, which contains important properties such as metrics, checkpoint info, and error details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c13bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = result.metrics\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b18221b",
   "metadata": {},
   "source": [
    "Expected output (your values may differ):\n",
    "\n",
    "```python\n",
    "OrderedDict([('train-logloss', 0.05463397157248817),\n",
    "             ('train-error', 0.00506329113924051),\n",
    "             ('validation-logloss', 0.06741214815308066),\n",
    "             ('validation-error', 0.01176470588235294)])\n",
    "```\n",
    "\n",
    "We see that the Ray Train logged metrics based on the values we configured in `eval_metric` and `evals`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e15f51a",
   "metadata": {},
   "source": [
    "We can also reconstruct the trained model from the checkpoint directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87892b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "booster = RayTrainReportCallback.get_model(result.checkpoint)\n",
    "booster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f0523a",
   "metadata": {},
   "source": [
    "## Model Registry\n",
    "\n",
    "Now that we've trained our model, let's save it to a model registry for future use. We'll use MLflow for this purpose, storing it in our [Anyscale user storage](https://docs.anyscale.com/configuration/storage/#user-storage). Ray also integrates with [other experiment trackers](https://docs.ray.io/en/latest/train/user-guides/experiment-tracking.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba23e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from dist_xgboost.constants import (\n",
    "    experiment_name,\n",
    "    model_fname,\n",
    "    model_registry,\n",
    "    preprocessor_fname,\n",
    ")\n",
    "\n",
    "# clean up old runs\n",
    "os.path.isdir(model_registry) and shutil.rmtree(model_registry)\n",
    "# mlflow.delete_experiment(experiment_name)\n",
    "os.makedirs(model_registry, exist_ok=True)\n",
    "\n",
    "\n",
    "# create a model registry in our user storage\n",
    "mlflow.set_tracking_uri(f\"file:{model_registry}\")\n",
    "\n",
    "# create a new experiment and log metrics and artifacts\n",
    "mlflow.set_experiment(experiment_name)\n",
    "with mlflow.start_run(description=\"xgboost breast cancer classifier on all features\"):\n",
    "    mlflow.log_params(model_config)\n",
    "    mlflow.log_metrics(metrics)\n",
    "\n",
    "    # Selectively log just the preprocessor and model weights\n",
    "    with TemporaryDirectory() as tmp_dir:\n",
    "        shutil.copy(\n",
    "            os.path.join(result.checkpoint.path, model_fname),\n",
    "            os.path.join(tmp_dir, model_fname),\n",
    "        )\n",
    "        shutil.copy(\n",
    "            preprocessor_path,\n",
    "            os.path.join(tmp_dir, preprocessor_fname),\n",
    "        )\n",
    "\n",
    "        mlflow.log_artifacts(tmp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340529d0",
   "metadata": {},
   "source": [
    "We can start the MLflow server to view our experiments:\n",
    "\n",
    "`mlflow server -h 0.0.0.0 -p 8080 --backend-store-uri {model_registry}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c529b2c",
   "metadata": {},
   "source": [
    "To view the dashboard, go to the **Overview tab** â†’ **Open Ports** â†’ `8080`.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-xgboost/refs/heads/main/images/mlflow.png\" width=685>\n",
    "\n",
    "You can also view the Ray Dashboard and Train workload dashboards:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-xgboost/refs/heads/main/images/train_metrics.png\" width=700>\n",
    "\n",
    "We can retrieve our best model from the registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d394cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dist_xgboost.data import get_best_model_from_registry\n",
    "\n",
    "best_model, artifacts_dir = get_best_model_from_registry()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f54394",
   "metadata": {},
   "source": [
    "### Production Deployment with Anyscale Jobs\n",
    "\n",
    "We can wrap our training workload as a production-grade [Anyscale Job](https://docs.anyscale.com/platform/jobs/) ([API ref](https://docs.anyscale.com/reference/job-api/)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb286b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Production batch job\n",
    "anyscale job submit --name=train-xboost-breast-cancer-model \\\n",
    "  --containerfile=\"/home/ray/default/containerfile\" \\\n",
    "  --working-dir=\"/home/ray/default\" \\\n",
    "  --exclude=\"\" \\\n",
    "  --max-retries=0 \\\n",
    "  -- python dist_xgboost/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557b6050",
   "metadata": {},
   "source": [
    "> **Note**: \n",
    "> - We're using a `containerfile` to define dependencies, but you could also use a pre-built image\n",
    "> - You can specify compute requirements as a [compute config](https://docs.anyscale.com/configuration/compute-configuration/) or inline in a [job config](https://docs.anyscale.com/reference/job-api#job-cli)\n",
    "> - When launched from a workspace without specifying compute, it defaults to the workspace's compute configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda2a7b0",
   "metadata": {},
   "source": [
    "## Scaling Strategies\n",
    "\n",
    "One of the key advantages of Ray Train is its ability to effortlessly scale your training workloads. By adjusting the [`ScalingConfig`](https://docs.ray.io/en/latest/train/api/doc/ray.train.ScalingConfig.html), you can optimize resource utilization and reduce training time.\n",
    "\n",
    "### Scaling Examples\n",
    "\n",
    "**Multi-node CPU Example** (4 nodes with 8 CPUs each):\n",
    "\n",
    "```python\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=4,\n",
    "    resources_per_worker={\"CPU\": 8},\n",
    ")\n",
    "```\n",
    "\n",
    "**Single-node multi-GPU Example** (1 node with 8 CPUs and 4 GPUs):\n",
    "\n",
    "```python\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=4,\n",
    "    use_gpu=True,\n",
    ")\n",
    "```\n",
    "\n",
    "**Multi-node multi-GPU Example** (4 nodes with 8 CPUs and 4 GPUs each):\n",
    "\n",
    "```python\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=16,\n",
    "    use_gpu=True,\n",
    ")\n",
    "```\n",
    "\n",
    "> **Important**: For multi-node clusters, you must specify a shared storage location (such as cloud storage or NFS) in the `run_config`. Using a local path will raise an error during checkpointing.\n",
    ">\n",
    "> ```python\n",
    "> trainer = XGBoostTrainer(\n",
    ">     ..., run_config=ray.train.RunConfig(storage_path=\"s3://...\")\n",
    "> )\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdab5180",
   "metadata": {},
   "source": [
    "### Worker Configuration Guidelines\n",
    "\n",
    "The optimal number of workers depends on your workload and cluster setup:\n",
    "\n",
    "- For **CPU-only training**, generally use one worker per node (XGBoost can leverage multiple CPUs with threading)\n",
    "- For **multi-GPU training**, use one worker per GPU\n",
    "- For **heterogeneous clusters**, consider the greatest common divisor of CPU counts\n",
    "\n",
    "### GPU Acceleration\n",
    "\n",
    "To use GPUs for training:\n",
    "\n",
    "1. Start one actor per GPU with `use_gpu=True`\n",
    "2. Set GPU-compatible parameters (e.g., `tree_method=\"gpu_hist\"` for XGBoost)\n",
    "3. Divide CPUs evenly across actors on each machine\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "trainer = XGBoostTrainer(\n",
    "    scaling_config=ScalingConfig(\n",
    "        # Number of workers to use for data parallelism.\n",
    "        num_workers=2,\n",
    "        # Whether to use GPU acceleration.\n",
    "        use_gpu=True,\n",
    "    ),\n",
    "    params={\n",
    "        # XGBoost specific params\n",
    "        \"tree_method\": \"gpu_hist\",  # GPU-specific parameter\n",
    "        \"eval_metric\": [\"logloss\", \"error\"],\n",
    "    },\n",
    "    ...\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021456cc",
   "metadata": {},
   "source": [
    "For more advanced topics, explore:\n",
    "- [Ray Tune](https://docs.ray.io/en/latest/tune/index.html) for hyperparameter optimization\n",
    "- [Ray Serve](https://docs.ray.io/en/latest/serve/index.html) for model deployment\n",
    "- [Ray Data](https://docs.ray.io/en/latest/data/data.html) for more advanced data processing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
