{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "612b6a05",
   "metadata": {},
   "source": [
    "# Distributed Training of an XGBoost Model on Anyscale\n",
    "\n",
    "\n",
    "<div align=\"left\">\n",
    "<a target=\"_blank\" href=\"https://console.anyscale.com/\"><img src=\"https://img.shields.io/badge/ðŸš€ Run_on-Anyscale-9hf\"></a>&nbsp;\n",
    "<a href=\"https://github.com/anyscale/e2e-xgboost\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&amp;message=View%20On%20GitHub&amp;color=586069&amp;logo=github&amp;labelColor=2f363d\"></a>&nbsp;\n",
    "</div>\n",
    "\n",
    "In this tutorial, we'll execute a distributed training workload that will connect the following heterogenous workloads:\n",
    "- Preprocessing the dataset with Ray Data\n",
    "- Distributed training of an XGBoost model with Ray Train\n",
    "- Saving model artifacts to a model registry (MLFlow)\n",
    "\n",
    "**Note**: We won't be tuning our model in this tutorial, but be sure to check out [Ray Tune](https://docs.ray.io/en/latest/tune/index.html) for experiment execution and hyperparameter tuning at any scale.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-xgboost/refs/heads/main/images/distributed_training.png\" width=800>\n",
    "\n",
    "\n",
    "Let's start by installing the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5493d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cba9f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable importing from dist_xgboost module\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ddcfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Ray Train v2. This will be the default in an upcoming release.\n",
    "os.environ[\"RAY_TRAIN_V2_ENABLED\"] = \"1\"\n",
    "# now it's safe to import from ray.train\n",
    "\n",
    "# Enable uv on Ray\n",
    "# https://docs.ray.io/en/latest/ray-core/handling-dependencies.html#using-uv-for-package-management\n",
    "os.environ[\"RAY_RUNTIME_ENV_HOOK\"] = \"ray._private.runtime_env.uv_runtime_env_hook.hook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cfb416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "from dist_xgboost.constants import local_storage_path, preprocessor_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f79e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make ray data less verbose\n",
    "ray.data.DataContext.get_current().enable_progress_bars = False\n",
    "ray.data.DataContext.get_current().print_on_execution_start = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad88db8",
   "metadata": {},
   "source": [
    "## Dataset Preparataion\n",
    "\n",
    "For this example, we're using the [\"Breast Cancer Wisconsin (Diagnostic)\"](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic) dataset, which contains features computed from digitized images of breast mass cell nuclei.\n",
    "\n",
    "We'll split the data into:\n",
    "- 70% for training\n",
    "- 15% for validation\n",
    "- 15% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1036655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data import Dataset\n",
    "\n",
    "\n",
    "def prepare_data() -> tuple[Dataset, Dataset, Dataset]:\n",
    "    \"\"\"Load and split the dataset into train, validation, and test sets.\"\"\"\n",
    "    # Load the dataset from S3\n",
    "    dataset = ray.data.read_csv(\"s3://anonymous@air-example-data/breast_cancer.csv\")\n",
    "    seed = 42\n",
    "\n",
    "    # Split 70% for training\n",
    "    train_dataset, rest = dataset.train_test_split(test_size=0.3, shuffle=True, seed=seed)\n",
    "    # Split the remaining 70% into 15% validation and 15% testing\n",
    "    valid_dataset, test_dataset = rest.train_test_split(test_size=0.5, shuffle=True, seed=seed)\n",
    "    return train_dataset, valid_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b0f220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 12:44:15,462\tINFO worker.py:1843 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-04-14 12:44:18,599\tINFO dataset.py:2796 -- Tip: Use `take_batch()` instead of `take() / show()` to return records in pandas or numpy batch format.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'mean radius': 12.34,\n",
       "  'mean texture': 26.86,\n",
       "  'mean perimeter': 81.15,\n",
       "  'mean area': 477.4,\n",
       "  'mean smoothness': 0.1034,\n",
       "  'mean compactness': 0.1353,\n",
       "  'mean concavity': 0.1085,\n",
       "  'mean concave points': 0.04562,\n",
       "  'mean symmetry': 0.1943,\n",
       "  'mean fractal dimension': 0.06937,\n",
       "  'radius error': 0.4053,\n",
       "  'texture error': 1.809,\n",
       "  'perimeter error': 2.642,\n",
       "  'area error': 34.44,\n",
       "  'smoothness error': 0.009098,\n",
       "  'compactness error': 0.03845,\n",
       "  'concavity error': 0.03763,\n",
       "  'concave points error': 0.01321,\n",
       "  'symmetry error': 0.01878,\n",
       "  'fractal dimension error': 0.005672,\n",
       "  'worst radius': 15.65,\n",
       "  'worst texture': 39.34,\n",
       "  'worst perimeter': 101.7,\n",
       "  'worst area': 768.9,\n",
       "  'worst smoothness': 0.1785,\n",
       "  'worst compactness': 0.4706,\n",
       "  'worst concavity': 0.4425,\n",
       "  'worst concave points': 0.1459,\n",
       "  'worst symmetry': 0.3215,\n",
       "  'worst fractal dimension': 0.1205,\n",
       "  'target': 0}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and split the dataset\n",
    "train_dataset, valid_dataset, _test_dataset = prepare_data()\n",
    "train_dataset.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65de1dd",
   "metadata": {},
   "source": [
    "Looking at the output, we can see the dataset contains features characterizing cell nuclei in breast mass, such as radius, texture, perimeter, area, smoothness, compactness, concavity, symmetry, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e67eb1",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Notice that the features have different magnitudes and ranges. While tree-based models like XGBoost aren't as sensitive to this, feature scaling can still improve numerical stability in some cases.\n",
    "\n",
    "Ray Data offers built-in preprocessors that simplify common feature preprocessing tasks, especially for tabular data. These can be seamlessly integrated with Ray Datasets, allowing you to preprocess your data in a fault-tolerant and distributed way.\n",
    "\n",
    "In this example, we'll use Ray's built-in `StandardScaler` to zero-center and normalize the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7256185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(columns=['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness', 'mean compactness', 'mean concavity', 'mean concave points', 'mean symmetry', 'mean fractal dimension', 'radius error', 'texture error', 'perimeter error', 'area error', 'smoothness error', 'compactness error', 'concavity error', 'concave points error', 'symmetry error', 'fractal dimension error', 'worst radius', 'worst texture', 'worst perimeter', 'worst area', 'worst smoothness', 'worst compactness', 'worst concavity', 'worst concave points', 'worst symmetry', 'worst fractal dimension'], output_columns=['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness', 'mean compactness', 'mean concavity', 'mean concave points', 'mean symmetry', 'mean fractal dimension', 'radius error', 'texture error', 'perimeter error', 'area error', 'smoothness error', 'compactness error', 'concavity error', 'concave points error', 'symmetry error', 'fractal dimension error', 'worst radius', 'worst texture', 'worst perimeter', 'worst area', 'worst smoothness', 'worst compactness', 'worst concavity', 'worst concave points', 'worst symmetry', 'worst fractal dimension'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.data.preprocessors import StandardScaler\n",
    "\n",
    "# Select all columns except the target for scaling\n",
    "columns_to_scale = [c for c in train_dataset.columns() if c != \"target\"]\n",
    "\n",
    "# Initialize the preprocessor\n",
    "preprocessor = StandardScaler(columns=columns_to_scale)\n",
    "# Fit the preprocessor on the training set only\n",
    "# (this prevents data leakage)\n",
    "preprocessor.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19daa596",
   "metadata": {},
   "source": [
    "Now that we've fit the preprocessor, we'll save it to a file. Later, we'll register this artifact in MLFlow so we can reuse it in downstream pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2688e721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(preprocessor_path, \"wb\") as f:\n",
    "    pickle.dump(preprocessor, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ff2165",
   "metadata": {},
   "source": [
    "Next, we'll transform our datasets using the fitted preprocessor. Note that the `transform()` operation is lazy - it won't be applied to the data until it's required by the train workers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230223b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'mean radius': -0.4610573754669751,\n",
       "  'mean texture': 1.7394058791227955,\n",
       "  'mean perimeter': -0.39862813331364333,\n",
       "  'mean area': -0.47052050327520184,\n",
       "  'mean smoothness': 0.5179183907087175,\n",
       "  'mean compactness': 0.6014975325986358,\n",
       "  'mean concavity': 0.27948609338625774,\n",
       "  'mean concave points': -0.03856734724040332,\n",
       "  'mean symmetry': 0.4255295258582904,\n",
       "  'mean fractal dimension': 0.8283748107392186,\n",
       "  'radius error': 0.05808719659016524,\n",
       "  'texture error': 0.9719565999844514,\n",
       "  'perimeter error': -0.06488976051695712,\n",
       "  'area error': -0.08812979470864253,\n",
       "  'smoothness error': 0.5966610931705398,\n",
       "  'compactness error': 0.6557505012910677,\n",
       "  'concavity error': 0.16691166236728125,\n",
       "  'concave points error': 0.2262776558930776,\n",
       "  'symmetry error': -0.24961481030565633,\n",
       "  'fractal dimension error': 0.6058568301981654,\n",
       "  'worst radius': -0.0801267576213063,\n",
       "  'worst texture': 2.1678368209602334,\n",
       "  'worst perimeter': -0.11584002773284043,\n",
       "  'worst area': -0.15607557768468996,\n",
       "  'worst smoothness': 2.0396930394107513,\n",
       "  'worst compactness': 1.3302986786233904,\n",
       "  'worst concavity': 0.8000233388201367,\n",
       "  'worst concave points': 0.510878227153386,\n",
       "  'worst symmetry': 0.4224928013250782,\n",
       "  'worst fractal dimension': 1.9408678428935453,\n",
       "  'target': 0}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = preprocessor.transform(train_dataset)\n",
    "valid_dataset = preprocessor.transform(valid_dataset)\n",
    "train_dataset.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ab598f",
   "metadata": {},
   "source": [
    "Using `take()`, we can see that the values are now zero-centered and rescaled to be roughly between -1 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128cb831",
   "metadata": {},
   "source": [
    "> **Data Processing Note**:  \n",
    "> For more advanced data loading and preprocessing techniques, check out the [comprehensive guide](https://docs.ray.io/en/latest/train/user-guides/data-loading-preprocessing.html). Ray Data also supports performant joins, filters, aggregations, and other operations for more structured data processing your workloads may require."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b534fa",
   "metadata": {},
   "source": [
    "## Model Training with XGBoost\n",
    "\n",
    "### Checkpointing Configuration\n",
    "\n",
    "Checkpointing is a powerful feature that enables you to resume training from the last checkpoint in case of interruptions. This is particularly useful for long-running training sessions.\n",
    "\n",
    "[`XGBoostTrainer`](https://docs.ray.io/en/latest/train/api/doc/ray.train.xgboost.XGBoostTrainer.html) implements checkpointing out of the box. We just need to configure [`CheckpointConfig`](https://docs.ray.io/en/latest/train/api/doc/ray.train.CheckpointConfig.html) to set the checkpointing frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9787bb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train import CheckpointConfig, Result, RunConfig, ScalingConfig\n",
    "\n",
    "# Configure checkpointing to save progress during training\n",
    "run_config = RunConfig(\n",
    "    checkpoint_config=CheckpointConfig(\n",
    "        # Checkpoint every 10 iterations\n",
    "        checkpoint_frequency=10,\n",
    "        # Only keep the latest checkpoint\n",
    "        num_to_keep=1,\n",
    "    ),\n",
    "    ## For multi-node clusters, configure storage that is accessible\n",
    "    ## across all worker nodes with `storage_path=\"s3://...\"`\n",
    "    storage_path=local_storage_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaee233",
   "metadata": {},
   "source": [
    "> **Note**: Once you enable checkpointing, you can follow [this guide](https://docs.ray.io/en/latest/train/user-guides/fault-tolerance.html) to enable fault tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9887b8e",
   "metadata": {},
   "source": [
    "### Training with XGBoost\n",
    "\n",
    "The training parameters are passed as a dictionary, similar to the original [`xgboost.train()`](https://xgboost.readthedocs.io/en/stable/parameter.html) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a173cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from ray.train.xgboost import RayTrainReportCallback, XGBoostTrainer\n",
    "\n",
    "\n",
    "def train_fn_per_worker(config: dict):\n",
    "    \"\"\"Training function that runs on each worker.\n",
    "\n",
    "    This function:\n",
    "    1. Gets the dataset shard for this worker\n",
    "    2. Converts to pandas for XGBoost\n",
    "    3. Separates features and labels\n",
    "    4. Creates DMatrix objects\n",
    "    5. Trains the model using distributed communication\n",
    "    \"\"\"\n",
    "    # Get this worker's dataset shard\n",
    "    train_ds, val_ds = (\n",
    "        ray.train.get_dataset_shard(\"train\"),\n",
    "        ray.train.get_dataset_shard(\"validation\"),\n",
    "    )\n",
    "\n",
    "    # Materialize the data and convert to pandas\n",
    "    train_ds = train_ds.materialize().to_pandas()\n",
    "    val_ds = val_ds.materialize().to_pandas()\n",
    "\n",
    "    # Separate the labels from the features\n",
    "    train_X, train_y = train_ds.drop(\"target\", axis=1), train_ds[\"target\"]\n",
    "    eval_X, eval_y = val_ds.drop(\"target\", axis=1), val_ds[\"target\"]\n",
    "\n",
    "    # Convert the data into DMatrix format for XGBoost\n",
    "    dtrain = xgboost.DMatrix(train_X, label=train_y)\n",
    "    deval = xgboost.DMatrix(eval_X, label=eval_y)\n",
    "\n",
    "    # Do distributed data-parallel training\n",
    "    # Ray Train sets up the necessary coordinator processes and\n",
    "    # environment variables for workers to communicate with each other\n",
    "    _booster = xgboost.train(\n",
    "        config[\"xgboost_params\"],\n",
    "        dtrain=dtrain,\n",
    "        evals=[(dtrain, \"train\"), (deval, \"validation\")],\n",
    "        num_boost_round=10,\n",
    "        # Handles metric logging and checkpointing\n",
    "        callbacks=[RayTrainReportCallback()],\n",
    "    )\n",
    "\n",
    "\n",
    "# Parameters for the XGBoost model\n",
    "model_config = {\n",
    "    \"xgboost_params\": {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": [\"logloss\", \"error\"],\n",
    "    }\n",
    "}\n",
    "\n",
    "trainer = XGBoostTrainer(\n",
    "    train_fn_per_worker,\n",
    "    train_loop_config=model_config,\n",
    "    # Register the data subsets\n",
    "    datasets={\"train\": train_dataset, \"validation\": valid_dataset},\n",
    "    # see \"How to scale out training?\" for more details\n",
    "    scaling_config=ScalingConfig(\n",
    "        # Number of workers for data parallelism.\n",
    "        num_workers=5,\n",
    "        # Set to True to use GPU acceleration\n",
    "        use_gpu=True,\n",
    "    ),\n",
    "    run_config=run_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70c0a68",
   "metadata": {},
   "source": [
    "> **Ray Train Benefits**:\n",
    "> \n",
    "> - **Multi-node orchestration**: Automatically handles multi-node, multi-GPU setup without manual SSH or hostfile configurations\n",
    "> - **Built-in fault tolerance**: Supports automatic retry of failed workers and can continue from the last checkpoint\n",
    "> - **Flexible training strategies**: Supports various parallelism strategies beyond just data parallel training\n",
    "> - **Heterogeneous cluster support**: Define per-worker resource requirements and run on mixed hardware\n",
    "> \n",
    "> Ray Train integrates with popular frameworks like PyTorch, TensorFlow, XGBoost, and more. For enterprise needs, [RayTurbo Train](https://docs.anyscale.com/rayturbo/rayturbo-train) offers additional features like elastic training, advanced monitoring, and performance optimization.\n",
    ">\n",
    "> <img src=\"https://raw.githubusercontent.com/anyscale/e2e-xgboost/refs/heads/main/images/train_integrations.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fe32cf",
   "metadata": {},
   "source": [
    "Now let's train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005f33bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TrainController pid=71051)\u001b[0m Attempting to start training worker group of size 5 with the following resources: [{'GPU': 1}] * 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +16s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
      "\u001b[33m(autoscaler +16s)\u001b[0m Error: No available node types can fulfill resource request {'GPU': 1.0}. Add suitable node types to this cluster to resolve this issue.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m result: Result = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/new_src/ANYSCALE/new_tutorials/e2e-xgboost-rebase/.venv/lib/python3.12/site-packages/ray/train/v2/api/data_parallel_trainer.py:112\u001b[39m, in \u001b[36mDataParallelTrainer.fit\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Launches the Ray Train controller to run training on workers.\u001b[39;00m\n\u001b[32m     96\u001b[39m \n\u001b[32m     97\u001b[39m \u001b[33;03mReturns:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m        `FailureConfig` is exhausted.\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    105\u001b[39m train_fn = construct_train_func(\n\u001b[32m    106\u001b[39m     \u001b[38;5;28mself\u001b[39m.train_loop_per_worker,\n\u001b[32m    107\u001b[39m     config=\u001b[38;5;28mself\u001b[39m.train_loop_config,\n\u001b[32m    108\u001b[39m     train_func_context=\u001b[38;5;28mself\u001b[39m.backend_config.train_func_context,\n\u001b[32m    109\u001b[39m     fn_arg_name=\u001b[33m\"\u001b[39m\u001b[33mtrain_loop_per_worker\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    110\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize_and_run_controller\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscaling_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_scaling_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaling_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfailure_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_failure_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfailure_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_run_context\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_run_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_default_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.error:\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# NOTE: If the training run errored out, raise an error back to the\u001b[39;00m\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# user's driver script.\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;66;03m# For example, if the Train `FailurePolicy` runs out of retries,\u001b[39;00m\n\u001b[32m    124\u001b[39m     \u001b[38;5;66;03m# and one of the workers errors. The controller will exit, and\u001b[39;00m\n\u001b[32m    125\u001b[39m     \u001b[38;5;66;03m# the error will be raised here.\u001b[39;00m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m result.error\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/new_src/ANYSCALE/new_tutorials/e2e-xgboost-rebase/.venv/lib/python3.12/site-packages/ray/train/v2/api/data_parallel_trainer.py:188\u001b[39m, in \u001b[36mDataParallelTrainer._initialize_and_run_controller\u001b[39m\u001b[34m(self, **controller_init_kwargs)\u001b[39m\n\u001b[32m    179\u001b[39m     controller_actor_cls = ray.remote(\n\u001b[32m    180\u001b[39m         num_cpus=\u001b[32m0\u001b[39m,\n\u001b[32m    181\u001b[39m         scheduling_strategy=NodeAffinitySchedulingStrategy(\n\u001b[32m   (...)\u001b[39m\u001b[32m    184\u001b[39m         runtime_env={\u001b[33m\"\u001b[39m\u001b[33menv_vars\u001b[39m\u001b[33m\"\u001b[39m: get_env_vars_to_propagate()},\n\u001b[32m    185\u001b[39m     )(TrainController)\n\u001b[32m    187\u001b[39m     controller = controller_actor_cls.remote(**controller_init_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m     \u001b[43mray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontroller\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m.\u001b[49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ray.get(controller.get_result.remote())\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/new_src/ANYSCALE/new_tutorials/e2e-xgboost-rebase/.venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py:21\u001b[39m, in \u001b[36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mauto_init_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     20\u001b[39m     auto_init_ray()\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/new_src/ANYSCALE/new_tutorials/e2e-xgboost-rebase/.venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py:103\u001b[39m, in \u001b[36mclient_mode_hook.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m func.\u001b[34m__name__\u001b[39m != \u001b[33m\"\u001b[39m\u001b[33minit\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[32m    102\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func.\u001b[34m__name__\u001b[39m)(*args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/new_src/ANYSCALE/new_tutorials/e2e-xgboost-rebase/.venv/lib/python3.12/site-packages/ray/_private/worker.py:2782\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(object_refs, timeout)\u001b[39m\n\u001b[32m   2776\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2777\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid type of object refs, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(object_refs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, is given. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2778\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mobject_refs\u001b[39m\u001b[33m'\u001b[39m\u001b[33m must either be an ObjectRef or a list of ObjectRefs. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2779\u001b[39m     )\n\u001b[32m   2781\u001b[39m \u001b[38;5;66;03m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2782\u001b[39m values, debugger_breakpoint = \u001b[43mworker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2783\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(values):\n\u001b[32m   2784\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayError):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/new_src/ANYSCALE/new_tutorials/e2e-xgboost-rebase/.venv/lib/python3.12/site-packages/ray/_private/worker.py:903\u001b[39m, in \u001b[36mWorker.get_objects\u001b[39m\u001b[34m(self, object_refs, timeout, return_exceptions, skip_deserialization)\u001b[39m\n\u001b[32m    893\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    894\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAttempting to call `get` on the value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobject_ref\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    895\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mwhich is not an ray.ObjectRef.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    896\u001b[39m         )\n\u001b[32m    898\u001b[39m timeout_ms = (\n\u001b[32m    899\u001b[39m     \u001b[38;5;28mint\u001b[39m(timeout * \u001b[32m1000\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m timeout != -\u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m -\u001b[32m1\u001b[39m\n\u001b[32m    900\u001b[39m )\n\u001b[32m    901\u001b[39m data_metadata_pairs: List[\n\u001b[32m    902\u001b[39m     Tuple[ray._raylet.Buffer, \u001b[38;5;28mbytes\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m ] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcore_worker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    904\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    908\u001b[39m debugger_breakpoint = \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    909\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m data, metadata \u001b[38;5;129;01min\u001b[39;00m data_metadata_pairs:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpython/ray/_raylet.pyx:3211\u001b[39m, in \u001b[36mray._raylet.CoreWorker.get_objects\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpython/ray/includes/common.pxi:83\u001b[39m, in \u001b[36mray._raylet.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "result: Result = trainer.fit()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf06ba2",
   "metadata": {},
   "source": [
    "Ray Train returns a [`ray.train.Result`](https://docs.ray.io/en/latest/train/api/doc/ray.train.Result.html) object, which contains important properties such as metrics, checkpoint info, and error details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c13bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = result.metrics\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b18221b",
   "metadata": {},
   "source": [
    "Expected output (your values may differ):\n",
    "\n",
    "```python\n",
    "OrderedDict([('train-logloss', 0.05463397157248817),\n",
    "             ('train-error', 0.00506329113924051),\n",
    "             ('validation-logloss', 0.06741214815308066),\n",
    "             ('validation-error', 0.01176470588235294)])\n",
    "```\n",
    "\n",
    "We see that the Ray Train logged metrics based on the values we configured in `eval_metric` and `evals`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e15f51a",
   "metadata": {},
   "source": [
    "We can also reconstruct the trained model from the checkpoint directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87892b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "booster = RayTrainReportCallback.get_model(result.checkpoint)\n",
    "booster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f0523a",
   "metadata": {},
   "source": [
    "## Model Registry\n",
    "\n",
    "Now that we've trained our model, let's save it to a model registry for future use. We'll use MLflow for this purpose, storing it in our [Anyscale user storage](https://docs.anyscale.com/configuration/storage/#user-storage). Ray also integrates with [other experiment trackers](https://docs.ray.io/en/latest/train/user-guides/experiment-tracking.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba23e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from dist_xgboost.constants import (\n",
    "    experiment_name,\n",
    "    model_fname,\n",
    "    model_registry,\n",
    "    preprocessor_fname,\n",
    ")\n",
    "\n",
    "# clean up old runs\n",
    "os.path.isdir(model_registry) and shutil.rmtree(model_registry)\n",
    "# mlflow.delete_experiment(experiment_name)\n",
    "os.makedirs(model_registry, exist_ok=True)\n",
    "\n",
    "\n",
    "# create a model registry in our user storage\n",
    "mlflow.set_tracking_uri(f\"file:{model_registry}\")\n",
    "\n",
    "# create a new experiment and log metrics and artifacts\n",
    "mlflow.set_experiment(experiment_name)\n",
    "with mlflow.start_run(description=\"xgboost breast cancer classifier on all features\"):\n",
    "    mlflow.log_params(model_config)\n",
    "    mlflow.log_metrics(metrics)\n",
    "\n",
    "    # Selectively log just the preprocessor and model weights\n",
    "    with TemporaryDirectory() as tmp_dir:\n",
    "        shutil.copy(\n",
    "            os.path.join(result.checkpoint.path, model_fname),\n",
    "            os.path.join(tmp_dir, model_fname),\n",
    "        )\n",
    "        shutil.copy(\n",
    "            preprocessor_path,\n",
    "            os.path.join(tmp_dir, preprocessor_fname),\n",
    "        )\n",
    "\n",
    "        mlflow.log_artifacts(tmp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340529d0",
   "metadata": {},
   "source": [
    "We can start the MLflow server to view our experiments:\n",
    "\n",
    "`mlflow server -h 0.0.0.0 -p 8080 --backend-store-uri {model_registry}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c529b2c",
   "metadata": {},
   "source": [
    "To view the dashboard, go to the **Overview tab** â†’ **Open Ports** â†’ `8080`.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-xgboost/refs/heads/main/images/mlflow.png\" width=685>\n",
    "\n",
    "You can also view the Ray Dashboard and Train workload dashboards:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-xgboost/refs/heads/main/images/train_metrics.png\" width=700>\n",
    "\n",
    "We can retrieve our best model from the registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d394cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dist_xgboost.data import get_best_model_from_registry\n",
    "\n",
    "best_model, artifacts_dir = get_best_model_from_registry()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f54394",
   "metadata": {},
   "source": [
    "### Production Deployment with Anyscale Jobs\n",
    "\n",
    "We can wrap our training workload as a production-grade [Anyscale Job](https://docs.anyscale.com/platform/jobs/) ([API ref](https://docs.anyscale.com/reference/job-api/)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb286b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Production batch job\n",
    "anyscale job submit --name=train-xboost-breast-cancer-model \\\n",
    "  --containerfile=\"/home/ray/default/containerfile\" \\\n",
    "  --working-dir=\"/home/ray/default\" \\\n",
    "  --exclude=\"\" \\\n",
    "  --max-retries=0 \\\n",
    "  -- python dist_xgboost/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557b6050",
   "metadata": {},
   "source": [
    "> **Note**: \n",
    "> - We're using a `containerfile` to define dependencies, but you could also use a pre-built image\n",
    "> - You can specify compute requirements as a [compute config](https://docs.anyscale.com/configuration/compute-configuration/) or inline in a [job config](https://docs.anyscale.com/reference/job-api#job-cli)\n",
    "> - When launched from a workspace without specifying compute, it defaults to the workspace's compute configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda2a7b0",
   "metadata": {},
   "source": [
    "## Scaling Strategies\n",
    "\n",
    "One of the key advantages of Ray Train is its ability to effortlessly scale your training workloads. By adjusting the [`ScalingConfig`](https://docs.ray.io/en/latest/train/api/doc/ray.train.ScalingConfig.html), you can optimize resource utilization and reduce training time.\n",
    "\n",
    "### Scaling Examples\n",
    "\n",
    "**Multi-node CPU Example** (4 nodes with 8 CPUs each):\n",
    "\n",
    "```python\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=4,\n",
    "    resources_per_worker={\"CPU\": 8},\n",
    ")\n",
    "```\n",
    "\n",
    "**Single-node multi-GPU Example** (1 node with 8 CPUs and 4 GPUs):\n",
    "\n",
    "```python\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=4,\n",
    "    use_gpu=True,\n",
    ")\n",
    "```\n",
    "\n",
    "**Multi-node multi-GPU Example** (4 nodes with 8 CPUs and 4 GPUs each):\n",
    "\n",
    "```python\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=16,\n",
    "    use_gpu=True,\n",
    ")\n",
    "```\n",
    "\n",
    "> **Important**: For multi-node clusters, you must specify a shared storage location (such as cloud storage or NFS) in the `run_config`. Using a local path will raise an error during checkpointing.\n",
    ">\n",
    "> ```python\n",
    "> trainer = XGBoostTrainer(\n",
    ">     ..., run_config=ray.train.RunConfig(storage_path=\"s3://...\")\n",
    "> )\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdab5180",
   "metadata": {},
   "source": [
    "### Worker Configuration Guidelines\n",
    "\n",
    "The optimal number of workers depends on your workload and cluster setup:\n",
    "\n",
    "- For **CPU-only training**, generally use one worker per node (XGBoost can leverage multiple CPUs with threading)\n",
    "- For **multi-GPU training**, use one worker per GPU\n",
    "- For **heterogeneous clusters**, consider the greatest common divisor of CPU counts\n",
    "\n",
    "### GPU Acceleration\n",
    "\n",
    "To use GPUs for training:\n",
    "\n",
    "1. Start one actor per GPU with `use_gpu=True`\n",
    "2. Set GPU-compatible parameters (e.g., `tree_method=\"gpu_hist\"` for XGBoost)\n",
    "3. Divide CPUs evenly across actors on each machine\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "trainer = XGBoostTrainer(\n",
    "    scaling_config=ScalingConfig(\n",
    "        # Number of workers to use for data parallelism.\n",
    "        num_workers=2,\n",
    "        # Whether to use GPU acceleration.\n",
    "        use_gpu=True,\n",
    "    ),\n",
    "    params={\n",
    "        # XGBoost specific params\n",
    "        \"tree_method\": \"gpu_hist\",  # GPU-specific parameter\n",
    "        \"eval_metric\": [\"logloss\", \"error\"],\n",
    "    },\n",
    "    ...\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021456cc",
   "metadata": {},
   "source": [
    "For more advanced topics, explore:\n",
    "- [Ray Tune](https://docs.ray.io/en/latest/tune/index.html) for hyperparameter optimization\n",
    "- [Ray Serve](https://docs.ray.io/en/latest/serve/index.html) for model deployment\n",
    "- [Ray Data](https://docs.ray.io/en/latest/data/data.html) for more advanced data processing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
