{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "612b6a05",
   "metadata": {},
   "source": [
    "# Distributed Training of an XGBoost Model on Anyscale\n",
    "\n",
    "\n",
    "<div align=\"left\">\n",
    "<a target=\"_blank\" href=\"https://console.anyscale.com/\"><img src=\"https://img.shields.io/badge/🚀 Run_on-Anyscale-9hf\"></a>&nbsp;\n",
    "<a href=\"https://github.com/anyscale/e2e-xgboost\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&amp;message=View%20On%20GitHub&amp;color=586069&amp;logo=github&amp;labelColor=2f363d\"></a>&nbsp;\n",
    "</div>\n",
    "\n",
    "In this tutorial, we'll execute a distributed training workload that will connect the following heterogenous workloads:\n",
    "- preprocess the dataset prior to training with Ray Data\n",
    "- distributed training with Ray Train\n",
    "- save model artifacts to a model registry (MLFlow)\n",
    "\n",
    "**Note**: we won't be tuning our model in this tutorial but be sure to check out [Ray Tune](https://docs.ray.io/en/latest/tune/index.html) for experiment execution and hyperparameter tuning at any scale.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-xgboost/refs/heads/main/images/distributed_training.png\" width=800>\n",
    "\n",
    "\n",
    "To run this tutorial, we need to install the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f5493d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b209489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the requirements are installed\n",
    "! pip install -qU -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cba9f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable importing from dist_xgboost module\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23ddcfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Ray Train v2. This will be the default in an upcoming release.\n",
    "os.environ[\"RAY_TRAIN_V2_ENABLED\"] = \"1\"\n",
    "# now it's safe to import from ray.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15cfb416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "from dist_xgboost.constants import preprocessor_path, local_storage_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05f79e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make ray data less verbose\n",
    "ray.data.DataContext.get_current().enable_progress_bars = False\n",
    "ray.data.DataContext.get_current().print_on_execution_start = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad88db8",
   "metadata": {},
   "source": [
    "Next we define a function to load our train, validation, and test datasets. For this example, we are using the [\"Breast Cancer Wisconsin (Diagnostic)\"](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic) dataset.\n",
    "\n",
    "We do this by first splitting a random 70% for the train subset, then dividing the remaining samples in half between the test set and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1036655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data import Dataset\n",
    "\n",
    "\n",
    "def prepare_data() -> tuple[Dataset, Dataset, Dataset]:\n",
    "    \"\"\"Load and split the dataset into train, validation, and test sets.\"\"\"\n",
    "    dataset = ray.data.read_csv(\"s3://anonymous@air-example-data/breast_cancer.csv\")\n",
    "    seed = 42\n",
    "    train_dataset, rest = dataset.train_test_split(\n",
    "        test_size=0.3, shuffle=True, seed=seed\n",
    "    )\n",
    "    # 15% for validation, 15% for testing\n",
    "    valid_dataset, test_dataset = rest.train_test_split(\n",
    "        test_size=0.5, shuffle=True, seed=seed\n",
    "    )\n",
    "    return train_dataset, valid_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b0f220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split the dataset\n",
    "train_dataset, valid_dataset, _test_dataset = prepare_data()\n",
    "train_dataset.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65de1dd",
   "metadata": {},
   "source": [
    "We see from the output that we have a tabular dataset [characterizing cell nuclei in breast mass](https://minds.wisconsin.edu/bitstream/handle/1793/59692/TR1131.pdf), such as radius, concavity, symmetry, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e67eb1",
   "metadata": {},
   "source": [
    "## How to preprocess data for training?\n",
    "\n",
    "Notice that the features have different magnitudes and ranges. This is stricly a problem for tree-based methods, however in some cases it can [improve numerical stability](https://stats.stackexchange.com/a/485681/80433).\n",
    "\n",
    "Ray Data offers built-in preprocessors that simplify common feature preprocessing tasks especially for tabular data.\n",
    "These can be seamlessly integrated with Ray Datasets, allowing you to preprocess your data in a fault-tolerant and distributed way before training.\n",
    "\n",
    "In this example, we use Ray's built-in StandardScaler to zero-center and z-score normalize the columns of our dataset. We fit the preprocessor using the train subset, and save it so that we can pre-process data when we deploy our model to production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7256185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.preprocessors import StandardScaler\n",
    "\n",
    "# pick some dataset columns to scale\n",
    "columns_to_scale = [c for c in train_dataset.columns() if c != \"target\"]\n",
    "\n",
    "# Initialize the preprocessor\n",
    "preprocessor = StandardScaler(columns=columns_to_scale)\n",
    "# train the preprocessor on the training set\n",
    "preprocessor.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19daa596",
   "metadata": {},
   "source": [
    "Now that we've fit the preprocessor, let's save it to a file. Later, we will register this artifact in MLFlow so that we can reuse it in downstream pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2688e721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(preprocessor_path, \"wb\") as f:\n",
    "    pickle.dump(preprocessor, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ff2165",
   "metadata": {},
   "source": [
    "Now that we have our preprocessor fitted, we can use it to transform our data. Note that this `transform()` operation is lazy; it won't be applied to the data until it is required by the train workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230223b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = preprocessor.transform(train_dataset)\n",
    "valid_dataset = preprocessor.transform(valid_dataset)\n",
    "train_dataset.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ab598f",
   "metadata": {},
   "source": [
    "Using `take()`, we can see that the values are now zero-centered and rescaled to be roughly between -1 and 1.\n",
    "\n",
    "Optionally, at this stage we could run the preprocessing step and save the intermediates using `ds.write_parquet(output_path)`. For the purposes of this example, we will just process the dataset in-memory in a streaming fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128cb831",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b> Data Processing</b> \n",
    "\n",
    "Be sure to checkout this extensive guide on [data loading and preprocessing](https://docs.ray.io/en/latest/train/user-guides/data-loading-preprocessing.html) for the last-mile preprocessing we'll need to do prior to training our models. However, Ray Data does support performant joins, filters, aggregations, etc. for the more structure data processing your workloads may need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b534fa",
   "metadata": {},
   "source": [
    "## Save and load XGBoost checkpoints\n",
    "\n",
    "Checkpointing is a powerful feature.\n",
    "It is particularly useful for long-running training sessions, as it enables you to resume training from the last checkpoint in case of interruptions.\n",
    "\n",
    "[`XGBoostTrainer`](https://docs.ray.io/en/latest/train/api/doc/ray.train.xgboost.XGBoostTrainer.html#ray.train.xgboost.XGBoostTrainer) implements checkpointing out of the box. These checkpoints can be loaded into memory\n",
    "using static methods [`XGBoostTrainer.get_model`](https://docs.ray.io/en/latest/train/api/doc/ray.train.xgboost.XGBoostTrainer.get_model.html#ray.train.xgboost.XGBoostTrainer.get_model).\n",
    "\n",
    "The only required change is to configure [`CheckpointConfig`](https://docs.ray.io/en/latest/train/api/doc/ray.train.CheckpointConfig.html#ray.train.CheckpointConfig) to set the checkpointing frequency. For example, the following configuration\n",
    "saves a checkpoint on every boosting round and only keeps the latest checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9787bb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train import CheckpointConfig, RunConfig, ScalingConfig, Result\n",
    "\n",
    "\n",
    "# Configure checkpointing to save progress during training\n",
    "run_config = RunConfig(\n",
    "    checkpoint_config=CheckpointConfig(\n",
    "        # Checkpoint every 10 iterations.\n",
    "        checkpoint_frequency=10,\n",
    "        # Only keep the latest checkpoint.\n",
    "        num_to_keep=1,\n",
    "    ),\n",
    "    ## If running in a multi-node cluster, this is where you\n",
    "    ## should configure the run's persistent storage that is accessible\n",
    "    ## across all worker nodes with `storage_path=\"s3://...\"`\n",
    "    storage_path=local_storage_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaee233",
   "metadata": {},
   "source": [
    ":::{tip} Once you enable checkpointing, you can follow [this guide](https://docs.ray.io/en/latest/train/user-guides/fault-tolerance.html#train-fault-tolerance) to enable fault tolerance. :::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9887b8e",
   "metadata": {},
   "source": [
    "## Basic training with tree-based models in Train\n",
    "\n",
    "Just as in the original [`xgboost.train()`](https://xgboost.readthedocs.io/en/stable/parameter.html) function, the training parameters are passed as the `params` dictionary.\n",
    "\n",
    "### XGBoost Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a173cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "from ray.train.xgboost import RayTrainReportCallback\n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "\n",
    "\n",
    "def train_fn_per_worker(config: dict):\n",
    "    # Get this worker's dataset shard convert\n",
    "    train_ds, val_ds = (\n",
    "        ray.train.get_dataset_shard(\"train\"),\n",
    "        ray.train.get_dataset_shard(\"validation\"),\n",
    "    )\n",
    "\n",
    "    train_ds = train_ds.materialize().to_pandas()\n",
    "    val_ds = val_ds.materialize().to_pandas()\n",
    "\n",
    "    # Separate the labels from the features\n",
    "    train_X, train_y = train_ds.drop(\"target\", axis=1), train_ds[\"target\"]\n",
    "    eval_X, eval_y = val_ds.drop(\"target\", axis=1), val_ds[\"target\"]\n",
    "\n",
    "    # Convert the data into a DMatrix\n",
    "    dtrain = xgboost.DMatrix(train_X, label=train_y)\n",
    "    deval = xgboost.DMatrix(eval_X, label=eval_y)\n",
    "\n",
    "    # Do distributed data-parallel training.\n",
    "    # Ray Train sets up the necessary coordinator processes and\n",
    "    # environment variables for your workers to communicate with each other.\n",
    "    # it also handles checkpointing via the `RayTrainReportCallback`\n",
    "    _booster = xgboost.train(\n",
    "        config[\"xgboost_params\"],\n",
    "        dtrain=dtrain,\n",
    "        evals=[(dtrain, \"train\"), (deval, \"validation\")],\n",
    "        num_boost_round=10,\n",
    "        callbacks=[RayTrainReportCallback()],\n",
    "    )\n",
    "\n",
    "\n",
    "# Params that will be passed to the base XGBoost model.\n",
    "model_config = {\n",
    "    \"xgboost_params\": {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": [\"logloss\", \"error\"],\n",
    "    }\n",
    "}\n",
    "\n",
    "trainer = XGBoostTrainer(\n",
    "    train_fn_per_worker,\n",
    "    train_loop_config=model_config,\n",
    "    # Register the data subsets.\n",
    "    datasets={\"train\": train_dataset, \"validation\": valid_dataset},\n",
    "    # see \"How to scale out training?\" for more details\n",
    "    scaling_config=ScalingConfig(\n",
    "        # Number of workers to use for data parallelism.\n",
    "        num_workers=5,\n",
    "        # Whether to use GPU acceleration. Set to True to schedule GPU workers.\n",
    "        use_gpu=True,\n",
    "    ),\n",
    "    run_config=run_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70c0a68",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b> Minimal change to your training code</b> \n",
    "\n",
    "You'll notice that there isn't much new Ray Train code on top of our base XGboost code. We specified how we want to scale out our training workload, load the Ray datasets and then checkpoint on our main worker node... and that's it! \n",
    "\n",
    "Check out these guides this extensive list of [Ray Train user guides](https://docs.ray.io/en/latest/train/user-guides.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7385197",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b> Ray Train</b> \n",
    "\n",
    "**🎛️ Multi-node orchestration made easy**\n",
    "\n",
    "- Ray Train automatically handles multi-node, multi-GPU setup with no manual SSH setup or hostfile configs. \n",
    "- And it also integrates with Ray's cluster launcher for cloud (AWS, GCP, K8s) and on-prem clusters. \n",
    "- Solutions like PyTorch DDP require manually setting up your own process group, ranks, networking, etc.\n",
    "\n",
    "**🩹 2. Built-in fault tolerance**\n",
    "- Ray Train supports automatic retry of failed workers.\n",
    "- and can continue training from the last checkpoint in case of failure.\n",
    "\n",
    "\n",
    "**✂️ 3. Flexible training strategies** (not just DDP)\n",
    "- Ray Train supports Data Parallel, Model Parallel, Parameter Server, and even custom strategies.\n",
    "- You can also use Torch DDP, FSPD, DeepSpeed, etc. under the hood if you want.\n",
    "- [Ray Compiled graphs](https://docs.ray.io/en/latest/ray-core/compiled-graph/ray-compiled-graph.html) allow us to even define different parallelism for jointly optimizing multipe models (Megatron, Deepspeed, etc. only allow for one global setting).\n",
    "\n",
    "**🔥 Better support for heterogeneous clusters**\n",
    "- Ray Train lets you define per-worker resource requirements (e.g., 2 CPUs and 1 GPU per worker).\n",
    "- and can run on heterogeneous machines and scale flexibly (e.g., CPU for preprocessing and GPU for training)\n",
    "\n",
    "**🌍 Integrations**\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/train_integrations.png\" width=500>\n",
    "\n",
    "[RayTurbo Train](https://docs.anyscale.com/rayturbo/rayturbo-train) offers even more improvement to the price-performance ratio, performance monitoring and more:\n",
    "- **elastic training** to scale to a dynamic number of workers, continue training on fewer resources (even on spot instances).\n",
    "- **purpose-built dashboard** designed to streamline the debugging of Ray Train workloads\n",
    "    - Monitoring: View the status of training runs and train workers.\n",
    "    - Metrics: See insights on training throughput, training system operation time.\n",
    "    - Profiling: Investigate bottlenecks, hangs, or errors from individual training worker processes.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/train_dashboard.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fe32cf",
   "metadata": {},
   "source": [
    "Finally, we can train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005f33bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result: Result = trainer.fit()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf06ba2",
   "metadata": {},
   "source": [
    "Ray Train returns a [`ray.train.Result`](https://docs.ray.io/en/latest/train/api/doc/ray.train.Result.html) object, which contains a few useful properties such as 'metrics', 'checkpoint', 'error', 'path', 'metrics_dataframe', 'best_checkpoints':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c13bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = result.metrics\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b18221b",
   "metadata": {},
   "source": [
    "This should output something like:\n",
    "\n",
    "```python\n",
    "OrderedDict([('train-logloss', 0.05463397157248817),\n",
    "             ('train-error', 0.00506329113924051),\n",
    "             ('validation-logloss', 0.06741214815308066),\n",
    "             ('validation-error', 0.01176470588235294)])\n",
    "```\n",
    "\n",
    "We see that the Ray Train logged metrics based on the values we configured in `eval_metric` and `evals`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e15f51a",
   "metadata": {},
   "source": [
    "Ray Train also automatically stored model checkpoints in the `result.checkpoint` directory. We can use `RayTrainReportCallback` to re-create our booster, which will become handy in the next few guides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87892b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "booster = RayTrainReportCallback.get_model(result.checkpoint)\n",
    "booster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f0523a",
   "metadata": {},
   "source": [
    "# Model registry\n",
    "\n",
    "We'll be creating a model registry in our [Anyscale user storage](https://docs.anyscale.com/configuration/storage/#user-storage) to save our model checkpoints to. We'll be using OSS mlflow but we can easily [set up other experiment trackers](https://docs.ray.io/en/latest/train/user-guides/experiment-tracking.html) with Ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba23e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from dist_xgboost.constants import experiment_name\n",
    "import shutil\n",
    "from tempfile import TemporaryDirectory\n",
    "from dist_xgboost.constants import (\n",
    "    model_registry,\n",
    "    model_fname,\n",
    "    preprocessor_fname,\n",
    ")\n",
    "\n",
    "# clean up old runs\n",
    "os.path.isdir(model_registry) and shutil.rmtree(model_registry)\n",
    "# mlflow.delete_experiment(experiment_name)\n",
    "os.makedirs(model_registry, exist_ok=True)\n",
    "\n",
    "\n",
    "# create a model registry in our user storage\n",
    "mlflow.set_tracking_uri(f\"file:{model_registry}\")\n",
    "\n",
    "# create a new experiment and log metrics and artifacts\n",
    "mlflow.set_experiment(experiment_name)\n",
    "with mlflow.start_run(description=\"xgboost breast cancer classifier on all features\"):\n",
    "    mlflow.log_params(model_config)\n",
    "    mlflow.log_metrics(metrics)\n",
    "\n",
    "    # Selectively log just the preprocessor and model weights\n",
    "    with TemporaryDirectory() as tmp_dir:\n",
    "        shutil.copy(\n",
    "            os.path.join(result.checkpoint.path, model_fname),\n",
    "            os.path.join(tmp_dir, model_fname),\n",
    "        )\n",
    "        shutil.copy(\n",
    "            preprocessor_path,\n",
    "            os.path.join(tmp_dir, preprocessor_fname),\n",
    "        )\n",
    "\n",
    "        mlflow.log_artifacts(tmp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340529d0",
   "metadata": {},
   "source": [
    "We can view our experiment metrics and model artifacts in our model registry. We're using OSS mlflow so we can run the server by pointing to our model registry location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61f2bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mlflow server -h 0.0.0.0 -p 8080 --backend-store-uri {model_registry}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c529b2c",
   "metadata": {},
   "source": [
    "We can view the dashboard by going to the **Overview tab** up top → **Open Ports** → `8080`.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-xgboost/refs/heads/main/images/mlflow.png\" width=685>\n",
    "\n",
    "We also have our Ray Dashboard and Train workfload specific dashboards above. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-xgboost/refs/heads/main/images/train_metrics.png\" width=700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d394cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dist_xgboost.data import get_best_model_from_registry\n",
    "\n",
    "best_model, artifacts_dir = get_best_model_from_registry()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f54394",
   "metadata": {},
   "source": [
    "And we can easily wrap our training workload as a production grade [Anyscale Job](https://docs.anyscale.com/platform/jobs/) ([API ref](https://docs.anyscale.com/reference/job-api/))\n",
    "\n",
    "**Note**: \n",
    "- we're using a `containerfile` to define our dependencies, but we could easily use a pre-built image as well.\n",
    "- we can specify the compute as a [compute config](https://docs.anyscale.com/configuration/compute-configuration/) or inline in a [job config](https://docs.anyscale.com/reference/job-api#job-cli) file.\n",
    "- when we don't specify compute and when launching from a workspace, this defaults to the compute configuration of the Workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb286b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Production batch job\n",
    "# FIXME use relative paths\n",
    "anyscale job submit --name=train-xboost-breast-cancer-model \\\n",
    "  --containerfile=\"../containerfile\" \\\n",
    "  --working-dir=\"/home/ray/default\" \\\n",
    "  --exclude=\"\" \\\n",
    "  --max-retries=0 \\\n",
    "  -- python dist_xgboost/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda2a7b0",
   "metadata": {},
   "source": [
    "## How to scale out training?\n",
    "\n",
    "One of the key advantages of using Ray Train is its ability to effortlessly scale your training workloads.\n",
    "By adjusting the [`ScalingConfig`](https://docs.ray.io/en/latest/train/api/doc/ray.train.ScalingConfig.html#ray.train.ScalingConfig),\n",
    "you can optimize resource utilization and reduce training time, making it ideal for large-scale machine learning tasks.\n",
    "\n",
    ":::{note}\n",
    "Ray Train doesn’t modify or otherwise alter the working of the underlying XGBoost or LightGBM distributed training algorithms. Ray only provides orchestration, data ingest and fault tolerance. For more information on GBDT distributed training, refer to [XGBoost documentation](https://xgboost.readthedocs.io/en/stable/) and [LightGBM documentation](https://lightgbm.readthedocs.io/en/latest/).\n",
    ":::\n",
    "\n",
    "### Multi-node CPU Example\n",
    "\n",
    "Setup: 4 nodes with 8 CPUs each.\n",
    "\n",
    "Use-case: To utilize all resources in multi-node training.\n",
    "\n",
    "```python\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=4,\n",
    "    resources_per_worker={\"CPU\": 8},\n",
    ")\n",
    "```\n",
    "\n",
    "### Single-node multi-GPU Example\n",
    "\n",
    "Setup: 1 node with 8 CPUs and 4 GPUs.\n",
    "\n",
    "Use-case: If you have a single node with multiple GPUs, you need to use\n",
    "distributed training to leverage all GPUs.\n",
    "\n",
    "```python\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=4,\n",
    "    use_gpu=True,\n",
    ")\n",
    "```\n",
    "\n",
    "### Multi-node multi-GPU Example\n",
    "\n",
    "Setup: 4 nodes with 8 CPUs and 4 GPUs each.\n",
    "\n",
    "Use-case: If you have multiple nodes with multiple GPUs, you need to\n",
    "schedule one worker per GPU.\n",
    "\n",
    "```python\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=16,\n",
    "    use_gpu=True,\n",
    ")\n",
    "```\n",
    "\n",
    "Note that you just have to adjust the number of workers. Ray handles everything else automatically.\n",
    "\n",
    "::: {warning}\n",
    "Specifying a *shared storage location* (such as cloud storage or NFS) is *optional* for single-node clusters, but it is **required for multi-node clusters**. Using a local path will [raise an error](https://docs.ray.io/en/latest/train/user-guides/persistent-storage.html#multinode-local-storage-warning) during checkpointing for multi-node clusters.\n",
    "\n",
    "```python\n",
    "trainer = XGBoostTrainer(\n",
    "    ..., run_config=ray.train.RunConfig(storage_path=\"s3://...\")\n",
    ")\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdab5180",
   "metadata": {},
   "source": [
    "## How many remote actors should you use?\n",
    "\n",
    "This depends on your workload and your cluster setup. Generally there is no inherent benefit of running more than one remote actor per node for CPU-only training. This is because XGBoost can already leverage multiple CPUs with threading.\n",
    "\n",
    "However, in some cases, you should consider some starting more than one actor per node:\n",
    "\n",
    "For **multi GPU training**, each GPU should have a separate remote actor. Thus, if your machine has 24 CPUs and 4 GPUs, you want to start 4 remote actors with 6 CPUs and 1 GPU each\n",
    "\n",
    "In a **heterogeneous cluster**, you might want to find the [greatest common divisor](https://en.wikipedia.org/wiki/Greatest_common_divisor) for the number of CPUs. For example, for a cluster with three nodes of 4, 8, and 12 CPUs, respectively, you should set the number of actors to 6 and the CPUs per actor to 4.\n",
    "\n",
    "## How to use GPUs for training?\n",
    "\n",
    "Ray Train enables multi-GPU training for XGBoost and LightGBM. The core backends automatically leverage NCCL2 for cross-device communication. All you have to do is to start one actor per GPU and set GPU-compatible parameters. For example, XGBoost’s `tree_method` to `gpu_hist`. See XGBoost documentation for more details.\n",
    "\n",
    "For instance, if you have 2 machines with 4 GPUs each, you want to start 8 workers, and set `use_gpu=True`. There is usually no benefit in allocating less (for example, 0.5) or more than one GPU per actor.\n",
    "\n",
    "You should divide the CPUs evenly across actors per machine, so if your machines have 16 CPUs in addition to the 4 GPUs, each actor should have 4 CPUs to use.\n",
    "\n",
    "```python\n",
    "trainer = XGBoostTrainer(\n",
    "    scaling_config=ScalingConfig(\n",
    "        # Number of workers to use for data parallelism.\n",
    "        num_workers=2,\n",
    "        # Whether to use GPU acceleration.\n",
    "        use_gpu=True,\n",
    "    ),\n",
    "    params={\n",
    "        # XGBoost specific params\n",
    "        \"tree_method\": \"gpu_hist\",\n",
    "        \"eval_metric\": [\"logloss\", \"error\"],\n",
    "    },\n",
    "    ...\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998a031e",
   "metadata": {},
   "source": [
    "## How to optimize XGBoost memory usage?\n",
    "\n",
    "XGBoost uses a compute-optimized data structure called `DMatrix` to store training data.\n",
    "However, converting a dataset to a `DMatrix` involves storing a complete copy of the data\n",
    "as well as intermediate conversions.\n",
    "On a 64-bit system the format is 64-bit floats. Depending on the system and original dataset dtype, \n",
    "this matrix can thus occupy more memory than the original dataset.\n",
    "\n",
    "The **peak memory usage** for CPU-based training is at least 3x the dataset size, assuming dtype `float32` on a 64-bit system, plus about **400,000 KiB** for other resources, like operating system requirements and storing of intermediate results.\n",
    "\n",
    "### Example\n",
    "\n",
    "- Machine type: AWS m5.xlarge (4 vCPUs, 16 GiB RAM)\n",
    "- Usable RAM: ~15,350,000 KiB\n",
    "- Dataset: 1,250,000 rows with 1024 features, dtype float32. Total size: 5,000,000 KiB\n",
    "- XGBoost DMatrix size: ~10,000,000 KiB\n",
    "\n",
    "This dataset fits exactly on this node for training.\n",
    "\n",
    "Note that the DMatrix size might be lower on a 32 bit system.\n",
    "\n",
    "### GPUs\n",
    "\n",
    "Generally, the same memory requirements exist for GPU-based training. Additionally, the GPU must have enough memory to hold the dataset.\n",
    "\n",
    "In the preceding example, the GPU must have at least 10,000,000 KiB (about 9.6 GiB) memory. However, empirical data shows that using a `DeviceQuantileDMatrix` seems to result in more peak GPU memory usage, possibly for intermediate storage when loading data (about 10%).\n",
    "\n",
    "### Best practices\n",
    "\n",
    "In order to reduce peak memory usage, consider the following suggestions:\n",
    "\n",
    "- Store data as `float32` or less. You often don’t need more precision is often, and keeping data in a smaller format helps reduce peak memory usage for initial data loading.\n",
    "- Pass the `dtype` when loading data from CSV. Otherwise, floating point values are loaded as `np.float64` per default, increasing peak memory usage by 33%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
