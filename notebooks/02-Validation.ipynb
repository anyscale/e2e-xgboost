{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "087d1891",
   "metadata": {},
   "source": [
    "# Model validation using offline batch inference\n",
    "\n",
    "<div align=\"left\">\n",
    "<a target=\"_blank\" href=\"https://console.anyscale.com/\"><img src=\"https://img.shields.io/badge/üöÄ Run_on-Anyscale-9hf\"></a>&nbsp;\n",
    "<a href=\"https://github.com/anyscale/e2e-xgboost\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&amp;message=View%20On%20GitHub&amp;color=586069&amp;logo=github&amp;labelColor=2f363d\"></a>&nbsp;\n",
    "</div>\n",
    "\n",
    "In this tutorial, we'll execute a batch inference workload that will connect the following heterogenous workloads:\n",
    "- distributed read from cloud storage\n",
    "- apply distributed preprocessing\n",
    "- batch inference\n",
    "- distributed aggregation of summary metrics\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-xgboost/refs/heads/main/images/batch_inference.png\" width=800>\n",
    "\n",
    "The above figure illustrates how different chunks of data can be processed concurrently at various stages of the pipeline. This parallel execution maximizes resource utilization and throughput.\n",
    "\n",
    "Note that this diagram is an simplification for various reasons:\n",
    "\n",
    "* Backpressure mechanisms may throttle upstream operators to prevent overwhelming downstream stages\n",
    "* Dynamic repartitioning often happens as data moves through the pipeline, changing block counts and sizes\n",
    "* Available resources change as the cluster autoscales\n",
    "* System failures may disrupt the clean sequential flow shown in the diagram\n",
    "* Etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59218309",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b> Ray Data Streaming Execution</b> \n",
    "\n",
    "‚ùå Traditional batch execution (non-streaming like Spark without pipelining, Sagemaker Batch):\n",
    "- Reads the entire dataset into memory or a persistent intermediate format.\n",
    "- Only then starts applying transformations (like .map, .filter, etc).\n",
    "- Higher memory pressure and startup latency.\n",
    "\n",
    "‚úÖ Streaming execution (Ray Data):\n",
    "- Starts processing chunks (\"blocks\") as they are loaded (no need to wait for entire dataset to load)\n",
    "- Reduces memory footprint (no OOMs) and speeds up time to first output\n",
    "- increase resource utilization by reducing idle time\n",
    "- Online-style inference pipelines with minimal latency\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-xgboost/refs/heads/main/images/streaming.gif\" width=700>\n",
    "\n",
    "**Note**: Ray Data is not a real-time stream processing engine like Flink or Kafka Streams. Instead, it's batch processing with streaming execution, which is especially useful for iterative ML workloads, ETL pipelines, and preprocessing before training or inference. In fact, Ray typically has a [**2-17x throughput improvement**](https://www.anyscale.com/blog/offline-batch-inference-comparing-ray-apache-spark-and-sagemaker#-results-of-throughput-from-experiments) over solutions like Spark and Sagemaker Batch Transform, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5493d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53176de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable importing from dist_xgboost package\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1b35e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Ray Train v2\n",
    "os.environ[\"RAY_TRAIN_V2_ENABLED\"] = \"1\"\n",
    "# now it's safe to import from ray.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3d72bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make Ray Data less verbose\n",
    "import ray\n",
    "\n",
    "ray.data.DataContext.get_current().enable_progress_bars = False\n",
    "ray.data.DataContext.get_current().print_on_execution_start = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8fa9ff",
   "metadata": {},
   "source": [
    "### Data ingestion\n",
    "\n",
    "We'll start by reading our data from a public cloud storage bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411bfe5d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b> ‚úçÔ∏è Distributed READ/WRITE</b> \n",
    "\n",
    "Ray Data supports a wide range of data sources for both [loading](https://docs.ray.io/en/latest/data/loading-data.html) and [saving](https://docs.ray.io/en/latest/data/saving-data.html), from generic binary files in cloud storage to structured data formats used by modern data platforms. In this example, we'll read data from a public S3 bucket that we've prepared with our dataset. This `read` operation ‚Äî like the `write` operation you'll see later‚Äîruns in a distributed fashion. As a result, the data is processed in parallel across the cluster and doesn't need to be loaded entirely into memory at once, making it scalable and memory-efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7816f41",
   "metadata": {},
   "source": [
    "## Validating our XGboost model using Ray Data\n",
    "\n",
    "In `notebooks/01-Distributed-Training.ipynb`, we trained a XGBoost model and stored it in our MLFlow artifact storage. Now, we can use it to make predictions on our hold-out set.\n",
    "\n",
    "First, let's load the test dataset using the same procedure as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871a792a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 13:46:00,301\tINFO worker.py:1709 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266 \u001b[39m\u001b[22m\n",
      "2025-04-09 13:46:03,084\tINFO dataset.py:2679 -- Tip: Use `take_batch()` instead of `take() / show()` to return records in pandas or numpy batch format.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'mean radius': 14.9,\n",
       "  'mean texture': 22.53,\n",
       "  'mean perimeter': 102.1,\n",
       "  'mean area': 685.0,\n",
       "  'mean smoothness': 0.09947,\n",
       "  'mean compactness': 0.2225,\n",
       "  'mean concavity': 0.2733,\n",
       "  'mean concave points': 0.09711,\n",
       "  'mean symmetry': 0.2041,\n",
       "  'mean fractal dimension': 0.06898,\n",
       "  'radius error': 0.253,\n",
       "  'texture error': 0.8749,\n",
       "  'perimeter error': 3.466,\n",
       "  'area error': 24.19,\n",
       "  'smoothness error': 0.006965,\n",
       "  'compactness error': 0.06213,\n",
       "  'concavity error': 0.07926,\n",
       "  'concave points error': 0.02234,\n",
       "  'symmetry error': 0.01499,\n",
       "  'fractal dimension error': 0.005784,\n",
       "  'worst radius': 16.35,\n",
       "  'worst texture': 27.57,\n",
       "  'worst perimeter': 125.4,\n",
       "  'worst area': 832.7,\n",
       "  'worst smoothness': 0.1419,\n",
       "  'worst compactness': 0.709,\n",
       "  'worst concavity': 0.9019,\n",
       "  'worst concave points': 0.2475,\n",
       "  'worst symmetry': 0.2866,\n",
       "  'worst fractal dimension': 0.1155,\n",
       "  'target': 0}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.data import Dataset\n",
    "\n",
    "\n",
    "def prepare_data() -> tuple[Dataset, Dataset, Dataset]:\n",
    "    \"\"\"Load and split the dataset into train, validation, and test sets.\"\"\"\n",
    "    dataset = ray.data.read_csv(\"s3://anonymous@air-example-data/breast_cancer.csv\")\n",
    "    seed = 42\n",
    "    train_dataset, rest = dataset.train_test_split(\n",
    "        test_size=0.3, shuffle=True, seed=seed\n",
    "    )\n",
    "    # 15% for validation, 15% for testing\n",
    "    valid_dataset, test_dataset = rest.train_test_split(\n",
    "        test_size=0.5, shuffle=True, seed=seed\n",
    "    )\n",
    "    return train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "\n",
    "_, _, test_dataset = prepare_data()\n",
    "test_dataset.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30093e79",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b>üí° Ray Data best practices</b>\n",
    "\n",
    "- **trigger lazy execution**: we use [`take`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.take.html) to trigger the exection because Ray has lazy execution mode, which is great for decreasing execution time and decreasing memory utilization. But, this means that an operation like take, count, write, etc. will be needed to actually execute our workflow DAG.\n",
    "- **`materialize` during development**: we use [`materialize`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.materialize.html) to execute and materialize our dataset into Ray's [shared memory object store memory](https://docs.ray.io/en/latest/ray-core/objects.html). This way, we save a checkpoint at this point and future operations on our dataset can start from this point (we won't rerun all operations on the dataset again from scratch). This is a conventient feature during development (especially in a stateful environment like Jupyter notebooks) because we can run from our saved checkpoints.\n",
    "- **shuffling strategies**: we've chosen to shuffle our dataset because it's all ordered by class, so we'll randomly shuffle the ordering of input files before reading. Ray Data also provides an extensive list of [shuffling strategies](https://docs.ray.io/en/latest/data/shuffling-data.html) such as local shuffles, per-epoch shuffles, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7eb41a",
   "metadata": {},
   "source": [
    "We need to transform the input data in the same way we did during training. Here, we fetch the preprocessor from the artifact registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136097f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dist_xgboost.data import get_best_model_from_registry\n",
    "from dist_xgboost.constants import preprocessor_fname\n",
    "import pickle\n",
    "\n",
    "\n",
    "best_run, best_artifacts_dir = get_best_model_from_registry()\n",
    "\n",
    "with open(os.path.join(best_artifacts_dir, preprocessor_fname), \"rb\") as f:\n",
    "    preprocessor = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d851437",
   "metadata": {},
   "source": [
    "Next, we will define the transformation step in our Ray Data pipeline. We could do this on each item of the dataset individually using `.map()`, but that would be inefficient. We could optimize this step by processing entire batches of data at once and using vecotrized operations.\n",
    "\n",
    "For this, we will use Ray Data's [`map_batches`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html) method paired with the [`Preprocessor.transform_batch`](https://docs.ray.io/en/latest/data/api/doc/ray.data.preprocessor.Preprocessor.transform_batch.html#ray.data.preprocessor.Preprocessor.transform_batch) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d87eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean radius': 0.28316742882286705, 'mean texture': 0.7523604985155783, 'mean perimeter': 0.48669896676905805, 'mean area': 0.1468368988849869, 'mean smoothness': 0.2369449961779808, 'mean compactness': 2.2320249533254315, 'mean concavity': 2.3565557480507966, 'mean concave points': 1.3269287289169034, 'mean symmetry': 0.7690746828912589, 'mean fractal dimension': 0.776477048769651, 'radius error': -0.5412079178215152, 'texture error': -0.6312569374264887, 'perimeter error': 0.3773420778631571, 'area error': -0.3322040297744336, 'smoothness error': -0.06207156278394198, 'compactness error': 1.8842687990223528, 'concavity error': 1.4011696908922329, 'concave points error': 1.6050000169029714, 'symmetry error': -0.6904479637201796, 'fractal dimension error': 0.644062455623064, 'worst radius': 0.06652007879384304, 'worst texture': 0.2951992750088203, 'worst perimeter': 0.6001736321032014, 'worst area': -0.04273924639797485, 'worst smoothness': 0.4222472095938187, 'worst compactness': 2.7952238903321938, 'worst concavity': 2.931554464921162, 'worst concave points': 2.0513581299918746, 'worst symmetry': -0.10791242534884364, 'worst fractal dimension': 1.6715410257114927, 'target': 0}\n"
     ]
    }
   ],
   "source": [
    "def transform_with_preprocessor(batch_df):\n",
    "    # The preprocessor does not know about the `target` column,\n",
    "    # so we need to remove it temporarily then add it back\n",
    "    target = batch_df.pop(\"target\")\n",
    "    transformed_features = preprocessor.transform_batch(batch_df)\n",
    "    transformed_features[\"target\"] = target\n",
    "    return transformed_features\n",
    "\n",
    "\n",
    "# Apply the transformation to each batch\n",
    "test_dataset = test_dataset.map_batches(\n",
    "    transform_with_preprocessor, batch_format=\"pandas\", batch_size=1000\n",
    ")\n",
    "test_dataset.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e21449",
   "metadata": {},
   "source": [
    "We now have our preprocessing pipeline defined and are ready to run batch inference to get the model predictions on the test set.\n",
    "\n",
    "Let's load the model from the artifact registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5859a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.xgboost import RayTrainReportCallback\n",
    "from ray.train import Checkpoint\n",
    "\n",
    "checkpoint = Checkpoint.from_directory(best_artifacts_dir)\n",
    "model = RayTrainReportCallback.get_model(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c41b340",
   "metadata": {},
   "source": [
    "Next, we actually run the inference step. We want to avoid repeatedly loading the model for each batch, so we define a reusable class that can recycle the XGboost model for different batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ef57ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Predictor:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, batch: pd.DataFrame) -> pd.DataFrame:\n",
    "        # remove the target column for inference\n",
    "        target = batch.pop(\"target\")\n",
    "        dmatrix = xgboost.DMatrix(batch)\n",
    "        predictions = self.model.predict(dmatrix)\n",
    "\n",
    "        results = pd.DataFrame({\"prediction\": predictions, \"target\": target})\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be7083f",
   "metadata": {},
   "source": [
    "Now, we can parallelize inference across replicas of the model. As before, we speed up this stage by processing the data in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3690ceb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 13:46:26,623\tWARNING actor_pool_map_operator.py:274 -- To ensure full parallelization across an actor pool of size 4, the Dataset should consist of at least 4 distinct blocks. Consider increasing the parallelism when creating the Dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prediction': 0.043412428349256516, 'target': 0}\n"
     ]
    }
   ],
   "source": [
    "test_predictions = test_dataset.map_batches(\n",
    "    Predictor,\n",
    "    fn_constructor_kwargs={\"model\": model},\n",
    "    concurrency=4,  # Number of model replicas\n",
    "    batch_format=\"pandas\",\n",
    ")\n",
    "\n",
    "test_predictions.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6dcc80",
   "metadata": {},
   "source": [
    "Now that we have a the predictions, it's time to evalaute the model's accuracy, precision, recall, and F1-score. For this, we need to calculate the number of true positives, true negatives, false positives, and false negatives across the test subset. We split this step into its own stage to give us the flexibility to run the previous stage on a different type of machine if we so wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e2205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def confusion_matrix_batch(batch, threshold=0.5):\n",
    "    # apply a threshold to the predictions to get binary labels\n",
    "    batch[\"prediction\"] = (batch[\"prediction\"] > threshold).astype(int)\n",
    "\n",
    "    result = {}\n",
    "    cm = confusion_matrix(batch[\"target\"], batch[\"prediction\"], labels=[0, 1])\n",
    "    result[\"TN\"] = cm[0, 0]\n",
    "    result[\"FP\"] = cm[0, 1]\n",
    "    result[\"FN\"] = cm[1, 0]\n",
    "    result[\"TP\"] = cm[1, 1]\n",
    "    return pd.DataFrame(result, index=[0])\n",
    "\n",
    "\n",
    "test_results = test_predictions.map_batches(\n",
    "    confusion_matrix_batch, batch_format=\"pandas\", batch_size=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09edcdb8",
   "metadata": {},
   "source": [
    "Finally, we can aggregate the confusion matrix results from all the batches in order to get the global counts. For this, we can use Ray Data's built-in `sum()` aggregator. Note that this will finally materialize the dataset and execute all the previously declared lazy transformerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5dbbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 13:46:29,014\tWARNING actor_pool_map_operator.py:274 -- To ensure full parallelization across an actor pool of size 4, the Dataset should consist of at least 4 distinct blocks. Consider increasing the parallelism when creating the Dataset.\n"
     ]
    }
   ],
   "source": [
    "# Sum all confusion matrix values across batches\n",
    "cm_sums = test_results.sum([\"TN\", \"FP\", \"FN\", \"TP\"])\n",
    "\n",
    "# Extract confusion matrix components\n",
    "tn = cm_sums[\"sum(TN)\"]\n",
    "fp = cm_sums[\"sum(FP)\"]\n",
    "fn = cm_sums[\"sum(FN)\"]\n",
    "tp = cm_sums[\"sum(TP)\"]\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "metrics = {\"precision\": precision, \"recall\": recall, \"f1\": f1, \"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae128ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results:\n",
      "precision: 0.9574\n",
      "recall: 1.0000\n",
      "f1: 0.9783\n",
      "accuracy: 0.9767\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation results:\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c8ec6b",
   "metadata": {},
   "source": [
    "This should output something like:\n",
    "\n",
    "```\n",
    "Validation results:\n",
    "precision: 0.9574\n",
    "recall: 1.0000\n",
    "f1: 0.9783\n",
    "accuracy: 0.9767\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1095f1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've demonstrated how to validate a machine learning model using Ray Data:\n",
    "\n",
    "1. We loaded the breast cancer dataset testing subset with distributed reads\n",
    "2. We transformed the dataset in a streaming fashion\n",
    "3. We created a validation pipeline using Ray Data's transformations to:\n",
    "   - Make predictions on the test data\n",
    "   - Calculate confusion matrix components (TP, TN, FP, FN) for each batch\n",
    "   - Aggregate results across all batches\n",
    "4. We computed key performance metrics\n",
    "\n",
    "Ray Data enables you to efficiently run this pipeline on terabyte scale datasets without changing your code.\n",
    "\n",
    "In the next notebook, we'll see how we can serve this XGBoost model for online inference jobs using Ray Serve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
