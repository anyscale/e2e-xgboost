{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef164c16",
   "metadata": {},
   "source": [
    "# Scalable online XGBoost inference with Ray Serve\n",
    "\n",
    "In this tutorial, we will show how to create a distributed service for our trained XGBoost model. This allows you to serve your model for real-time predictions in a scalable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5493d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d16364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable loading of the dist_xgboost module\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34bfa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Ray Train v2\n",
    "os.environ[\"RAY_TRAIN_V2_ENABLED\"] = \"1\"\n",
    "# now it's safe to import from ray.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36adb412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make ray data less verbose\n",
    "import ray\n",
    "\n",
    "ray.data.DataContext.get_current().enable_progress_bars = False\n",
    "ray.data.DataContext.get_current().print_on_execution_start = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a5b2ba",
   "metadata": {},
   "source": [
    "## Loading the Model\n",
    "\n",
    "Next, we load the pre-trained preprocessor and XGBoost model from the MLFlow registry as we demonstrated in the validation notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5c110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dist_xgboost.data import get_best_model_from_registry\n",
    "from dist_xgboost.constants import preprocessor_fname\n",
    "import pickle\n",
    "from ray.train.xgboost import RayTrainReportCallback\n",
    "from ray.train import Checkpoint\n",
    "\n",
    "\n",
    "best_run, best_artifacts_dir = get_best_model_from_registry()\n",
    "\n",
    "with open(os.path.join(best_artifacts_dir, preprocessor_fname), \"rb\") as f:\n",
    "    preprocessor = pickle.load(f)\n",
    "\n",
    "\n",
    "checkpoint = Checkpoint.from_directory(best_artifacts_dir)\n",
    "model = RayTrainReportCallback.get_model(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf58fa92",
   "metadata": {},
   "source": [
    "## Creating a Ray Serve Deployment\n",
    "\n",
    "We'll now define our Ray Serve endpoint. We'll use a reusable class to avoid reloading the model and preprocessor for each request. Our deployment will support both Pythonic and HTTP requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb59a703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import serve\n",
    "from starlette.requests import Request\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "class XGBoostModel:\n",
    "    def __init__(self, preprocessor, model):\n",
    "        self.model = model\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def pythonic_call(self, input_data: dict) -> dict:\n",
    "        # Convert to DataFrame\n",
    "        input_df = pd.DataFrame([input_data])\n",
    "        # Preprocess the input\n",
    "        preprocessed_batch = self.preprocessor.transform_batch(input_df)\n",
    "        # Create DMatrix for prediction\n",
    "        dmatrix = xgboost.DMatrix(preprocessed_batch)\n",
    "        # Get predictions\n",
    "        predictions = self.model.predict(dmatrix)\n",
    "        return {\"predictions\": predictions.tolist()}\n",
    "\n",
    "    async def __call__(self, request: Request) -> dict:\n",
    "        # Parse the request body as JSON\n",
    "        input_data = await request.json()\n",
    "        return self.pythonic_call(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06e9701",
   "metadata": {},
   "source": [
    "Let's ensure that we don't have any existing deployments first using [`serve.shutdown()`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.shutdown.html#ray.serve.shutdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfec0c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 14:27:30,601\tINFO worker.py:1709 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8267 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "if (\n",
    "    \"default\" in serve.status().applications\n",
    "    and serve.status().applications[\"default\"].status == \"RUNNING\"\n",
    "):\n",
    "    print(\"Shutting down existing serve application\")\n",
    "    serve.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b67d6ca",
   "metadata": {},
   "source": [
    "Now that we've defined the deployment, we can create our `ray.serve.Application` using the [`.bind()`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.Deployment.html#ray.serve.Deployment) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd2bbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model = XGBoostModel.bind(preprocessor, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a9892e",
   "metadata": {},
   "source": [
    "## Preparing Test Data\n",
    "\n",
    "Let's prepare some example data to test our deployment. We'll use a sample from our hold-out set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ba658c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = {\n",
    "    \"mean radius\": 14.9,\n",
    "    \"mean texture\": 22.53,\n",
    "    \"mean perimeter\": 102.1,\n",
    "    \"mean area\": 685.0,\n",
    "    \"mean smoothness\": 0.09947,\n",
    "    \"mean compactness\": 0.2225,\n",
    "    \"mean concavity\": 0.2733,\n",
    "    \"mean concave points\": 0.09711,\n",
    "    \"mean symmetry\": 0.2041,\n",
    "    \"mean fractal dimension\": 0.06898,\n",
    "    \"radius error\": 0.253,\n",
    "    \"texture error\": 0.8749,\n",
    "    \"perimeter error\": 3.466,\n",
    "    \"area error\": 24.19,\n",
    "    \"smoothness error\": 0.006965,\n",
    "    \"compactness error\": 0.06213,\n",
    "    \"concavity error\": 0.07926,\n",
    "    \"concave points error\": 0.02234,\n",
    "    \"symmetry error\": 0.01499,\n",
    "    \"fractal dimension error\": 0.005784,\n",
    "    \"worst radius\": 16.35,\n",
    "    \"worst texture\": 27.57,\n",
    "    \"worst perimeter\": 125.4,\n",
    "    \"worst area\": 832.7,\n",
    "    \"worst smoothness\": 0.1419,\n",
    "    \"worst compactness\": 0.709,\n",
    "    \"worst concavity\": 0.9019,\n",
    "    \"worst concave points\": 0.2475,\n",
    "    \"worst symmetry\": 0.2866,\n",
    "    \"worst fractal dimension\": 0.1155,\n",
    "}\n",
    "sample_target = 0  # Ground truth label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c9b08c",
   "metadata": {},
   "source": [
    "## Running the Service\n",
    "\n",
    "There are two ways to run a Ray Serve service:\n",
    "\n",
    "1) **Serve API**:  use the [`serve run`](https://docs.ray.io/en/latest/serve/getting_started.html#running-a-ray-serve-application) CLI command, e.g. `serve run tutorial:xgboost_model`\n",
    "2) **Pythonic API**: use `ray.serve`'s [`serve.run` command](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.run.html#ray.serve.run), e.g. `serve.run(xgboost_model)`.\n",
    "\n",
    "For this example, we'll use the Pythonic API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03964807",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-04-09 14:27:31,954 serve 42336 -- Started Serve in namespace \"serve\".\n",
      "INFO 2025-04-09 14:27:33,066 serve 42336 -- Application 'xgboost-breast-cancer-classifier' is ready at http://127.0.0.1:8000/.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ProxyActor pid=42374)\u001b[0m INFO 2025-04-09 14:27:31,923 proxy 127.0.0.1 -- Proxy starting on node 94fbfec9b89f75d90b7512b30d7076cf04fcad9cb384734bcd49a346 (HTTP port: 8000).\n",
      "\u001b[36m(ProxyActor pid=42374)\u001b[0m INFO 2025-04-09 14:27:31,942 proxy 127.0.0.1 -- Got updated endpoints: {}.\n",
      "\u001b[36m(ServeController pid=42385)\u001b[0m INFO 2025-04-09 14:27:31,964 controller 42385 -- Deploying new version of Deployment(name='XGBoostModel', app='xgboost-breast-cancer-classifier') (initial target replicas: 1).\n",
      "\u001b[36m(ProxyActor pid=42374)\u001b[0m INFO 2025-04-09 14:27:31,965 proxy 127.0.0.1 -- Got updated endpoints: {Deployment(name='XGBoostModel', app='xgboost-breast-cancer-classifier'): EndpointInfo(route='/', app_is_cross_language=False)}.\n",
      "\u001b[36m(ProxyActor pid=42374)\u001b[0m INFO 2025-04-09 14:27:31,969 proxy 127.0.0.1 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x10e8c0350>.\n",
      "\u001b[36m(ServeController pid=42385)\u001b[0m INFO 2025-04-09 14:27:32,066 controller 42385 -- Adding 1 replica to Deployment(name='XGBoostModel', app='xgboost-breast-cancer-classifier').\n",
      "\u001b[36m(ServeReplica:xgboost-breast-cancer-classifier:XGBoostModel pid=42381)\u001b[0m INFO 2025-04-09 14:27:44,082 xgboost-breast-cancer-classifier_XGBoostModel bwkqlb6a 0c8c5cc8-1b2a-4490-89e5-7b9a92e7e839 -- POST / 200 6.8ms\n",
      "\u001b[36m(ServeReplica:xgboost-breast-cancer-classifier:XGBoostModel pid=42381)\u001b[0m /Users/rdecal/new_src/ANYSCALE/new_tutorials/e2e-xgboost-rebase/.venv/lib/python3.12/site-packages/ray/serve/_private/replica.py:1173: UserWarning: Calling sync method 'pythonic_call' directly on the asyncio loop. In a future version, sync methods will be run in a threadpool by default. Ensure your sync methods are thread safe or keep the existing behavior by making them `async def`. Opt into the new behavior by setting RAY_SERVE_RUN_SYNC_IN_THREADPOOL=1.\n",
      "\u001b[36m(ServeReplica:xgboost-breast-cancer-classifier:XGBoostModel pid=42381)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(ServeReplica:xgboost-breast-cancer-classifier:XGBoostModel pid=42381)\u001b[0m INFO 2025-04-09 14:27:45,126 xgboost-breast-cancer-classifier_XGBoostModel bwkqlb6a 70d563cc-6be5-4f4b-a36d-82c5cc18d0e9 -- CALL pythonic_call OK 5.2ms\n"
     ]
    }
   ],
   "source": [
    "from ray.serve.handle import DeploymentHandle\n",
    "\n",
    "handle: DeploymentHandle = serve.run(\n",
    "    xgboost_model, name=\"xgboost-breast-cancer-classifier\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e2b045",
   "metadata": {},
   "source": [
    "We should see some logs indicating that the service is running locally:\n",
    "\n",
    "```bash\n",
    "INFO 2025-04-09 14:06:55,760 serve 31684 -- Started Serve in namespace \"serve\".\n",
    "INFO 2025-04-09 14:06:57,875 serve 31684 -- Application 'default' is ready at http://127.0.0.1:8000/.\n",
    "```\n",
    "\n",
    "We can also check whether it is running using [`serve.status()`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.status.html#ray.serve.status):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72794309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serve.status().applications[\"xgboost-breast-cancer-classifier\"].status == \"RUNNING\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060b974c",
   "metadata": {},
   "source": [
    "## Querying the Service\n",
    "\n",
    "### Using HTTP\n",
    "The most common way to query services is via an HTTP request. This invokes the `__call__` method we defined earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d99974c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 0.0434\n",
      "Ground truth: 0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://127.0.0.1:8000/\"\n",
    "response = requests.post(url, json=sample_input).json()\n",
    "\n",
    "print(f\"Prediction: {response['predictions'][0]:.4f}\")\n",
    "print(f\"Ground truth: {sample_target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff978c09",
   "metadata": {},
   "source": [
    "### Using Python\n",
    "\n",
    "For a more direct Pythonic way to query the model, you can use the deployment handle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96becd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-04-09 14:27:45,116 serve 42336 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x15f230110>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [0.043412428349256516]}\n"
     ]
    }
   ],
   "source": [
    "response = await handle.pythonic_call.remote(sample_input)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a358e256",
   "metadata": {},
   "source": [
    "This approach is useful if you need to interact with the service from a different process in the same Ray Cluster. If you need to regenerate the serve handle, you can use [`serve.get_deployment_handle`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.get_deployment_handle.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8408d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = serve.get_deployment_handle(\"XGBoostModel\", \"xgboost-breast-cancer-classifier\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
