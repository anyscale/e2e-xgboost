{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef164c16",
   "metadata": {},
   "source": [
    "# Scalable online XGBoost inference with Ray Serve\n",
    "\n",
    "<div align=\"left\">\n",
    "<a target=\"_blank\" href=\"https://console.anyscale.com/\"><img src=\"https://img.shields.io/badge/ðŸš€ Run_on-Anyscale-9hf\"></a>&nbsp;\n",
    "<a href=\"https://github.com/anyscale/e2e-xgboost\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&amp;message=View%20On%20GitHub&amp;color=586069&amp;logo=github&amp;labelColor=2f363d\"></a>&nbsp;\n",
    "</div>\n",
    "\n",
    "In this tutorial, we'll launch an online service that will:\n",
    "- deploy our trained XGBoost model artifacts to generate predictions\n",
    "- autoscale based on real-time incoming traffic\n",
    "- cover observability and debugging around our service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459a66c4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\">\n",
    "\n",
    "[Ray Serve](https://docs.ray.io/en/latest/serve/index.html) is a highly scalable and flexible model serving library for building online inference APIs.\n",
    "- wrap our models and business logic as separate [serve deployments](https://docs.ray.io/en/latest/serve/key-concepts.html#deployment) and [connect](https://docs.ray.io/en/latest/serve/model_composition.html) them together (pipeline, ensemble, etc.)\n",
    "- avoid one large service that network and compute bounded (inefficient use of resources)\n",
    "- utilize fractional heterogenous [resources](https://docs.ray.io/en/latest/serve/resource-allocation.html) (**not possible** with Sagemaker, Vertex, KServe, etc.) and horizontally scale (`num_replicas`)\n",
    "- [autoscale](https://docs.ray.io/en/latest/serve/autoscaling-guide.html) up/down based on traffic\n",
    "- integrations with [FastAPI and HTTP](https://docs.ray.io/en/latest/serve/http-guide.html)\n",
    "- set up a [gRPC service](https://docs.ray.io/en/latest/serve/advanced-guides/grpc-guide.html#set-up-a-grpc-service) to build distributed systems and microservices.\n",
    "- enable [dynamic batching](https://docs.ray.io/en/latest/serve/advanced-guides/dyn-req-batch.html) (based on batch size, time, etc.)\n",
    "- suite of [utilities for serving LLMs](https://docs.ray.io/en/latest/serve/llm/serving-llms.html) (multi-lora, inference engine agnostic, etc.)\n",
    "\n",
    "<img src=\"https://github.com/anyscale/e2e-xgboost/blob/main/images/ray_serve.png?raw=true\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5493d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d16364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable loading of the dist_xgboost module\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34bfa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Ray Train v2\n",
    "os.environ[\"RAY_TRAIN_V2_ENABLED\"] = \"1\"\n",
    "# now it's safe to import from ray.train\n",
    "\n",
    "# Enable uv on Ray\n",
    "# https://docs.ray.io/en/latest/ray-core/handling-dependencies.html#using-uv-for-package-management\n",
    "os.environ[\"RAY_RUNTIME_ENV_HOOK\"] = \"ray._private.runtime_env.uv_runtime_env_hook.hook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36adb412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make ray data less verbose\n",
    "import ray\n",
    "\n",
    "ray.data.DataContext.get_current().enable_progress_bars = False\n",
    "ray.data.DataContext.get_current().print_on_execution_start = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a5b2ba",
   "metadata": {},
   "source": [
    "## Loading the Model\n",
    "\n",
    "Next, we load the pre-trained preprocessor and XGBoost model from the MLFlow registry as we demonstrated in the validation notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf58fa92",
   "metadata": {},
   "source": [
    "## Creating a Ray Serve Deployment\n",
    "\n",
    "We'll now define our Ray Serve endpoint. We'll use a reusable class to avoid reloading the model and preprocessor for each request. Our deployment will support both Pythonic and HTTP requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb59a703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost\n",
    "from ray import serve\n",
    "from starlette.requests import Request\n",
    "\n",
    "from dist_xgboost.data import load_model_and_preprocessor\n",
    "\n",
    "\n",
    "@serve.deployment(num_replicas=2, ray_actor_options={\"num_cpus\": 2})\n",
    "class XGBoostModel:\n",
    "    def __init__(self):\n",
    "        self.preprocessor, self.model = load_model_and_preprocessor()\n",
    "\n",
    "    def pythonic_call(self, input_data: dict) -> dict:\n",
    "        # Convert to DataFrame\n",
    "        input_df = pd.DataFrame([input_data])\n",
    "        # Preprocess the input\n",
    "        preprocessed_batch = self.preprocessor.transform_batch(input_df)\n",
    "        # Create DMatrix for prediction\n",
    "        dmatrix = xgboost.DMatrix(preprocessed_batch)\n",
    "        # Get predictions\n",
    "        predictions = self.model.predict(dmatrix)\n",
    "        return {\"predictions\": predictions.tolist()}\n",
    "\n",
    "    async def __call__(self, request: Request) -> dict:\n",
    "        # Parse the request body as JSON\n",
    "        input_data = await request.json()\n",
    "        return self.pythonic_call(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cc6588",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b>ðŸ§± Model composition</b>\n",
    "\n",
    "Ray Serve makes it extremely easy to do [model composition](https://docs.ray.io/en/latest/serve/model_composition.html) where we can compose multiple deployments containing ML models or business logic into a single application. And we can independently scale (even fractional resources) and configure each of our deployments.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/serve_composition.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06e9701",
   "metadata": {},
   "source": [
    "Let's ensure that we don't have any existing deployments first using [`serve.shutdown()`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.shutdown.html#ray.serve.shutdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfec0c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 14:27:30,601\tINFO worker.py:1709 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8267 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "if \"default\" in serve.status().applications and serve.status().applications[\"default\"].status == \"RUNNING\":\n",
    "    print(\"Shutting down existing serve application\")\n",
    "    serve.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b67d6ca",
   "metadata": {},
   "source": [
    "Now that we've defined the deployment, we can create our `ray.serve.Application` using the [`.bind()`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.Deployment.html#ray.serve.Deployment) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecd2bbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the app\n",
    "xgboost_model = XGBoostModel.bind()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a9892e",
   "metadata": {},
   "source": [
    "## Preparing Test Data\n",
    "\n",
    "Let's prepare some example data to test our deployment. We'll use a sample from our hold-out set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ba658c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = {\n",
    "    \"mean radius\": 14.9,\n",
    "    \"mean texture\": 22.53,\n",
    "    \"mean perimeter\": 102.1,\n",
    "    \"mean area\": 685.0,\n",
    "    \"mean smoothness\": 0.09947,\n",
    "    \"mean compactness\": 0.2225,\n",
    "    \"mean concavity\": 0.2733,\n",
    "    \"mean concave points\": 0.09711,\n",
    "    \"mean symmetry\": 0.2041,\n",
    "    \"mean fractal dimension\": 0.06898,\n",
    "    \"radius error\": 0.253,\n",
    "    \"texture error\": 0.8749,\n",
    "    \"perimeter error\": 3.466,\n",
    "    \"area error\": 24.19,\n",
    "    \"smoothness error\": 0.006965,\n",
    "    \"compactness error\": 0.06213,\n",
    "    \"concavity error\": 0.07926,\n",
    "    \"concave points error\": 0.02234,\n",
    "    \"symmetry error\": 0.01499,\n",
    "    \"fractal dimension error\": 0.005784,\n",
    "    \"worst radius\": 16.35,\n",
    "    \"worst texture\": 27.57,\n",
    "    \"worst perimeter\": 125.4,\n",
    "    \"worst area\": 832.7,\n",
    "    \"worst smoothness\": 0.1419,\n",
    "    \"worst compactness\": 0.709,\n",
    "    \"worst concavity\": 0.9019,\n",
    "    \"worst concave points\": 0.2475,\n",
    "    \"worst symmetry\": 0.2866,\n",
    "    \"worst fractal dimension\": 0.1155,\n",
    "}\n",
    "sample_target = 0  # Ground truth label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c9b08c",
   "metadata": {},
   "source": [
    "## Running the Service\n",
    "\n",
    "There are two ways to run a Ray Serve service:\n",
    "\n",
    "1) **Serve API**:  use the [`serve run`](https://docs.ray.io/en/latest/serve/getting_started.html#running-a-ray-serve-application) CLI command, e.g. `serve run tutorial:xgboost_model`\n",
    "2) **Pythonic API**: use `ray.serve`'s [`serve.run` command](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.run.html#ray.serve.run), e.g. `serve.run(xgboost_model)`.\n",
    "\n",
    "For this example, we'll use the Pythonic API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03964807",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-04-09 14:27:31,954 serve 42336 -- Started Serve in namespace \"serve\".\n",
      "INFO 2025-04-09 14:27:33,066 serve 42336 -- Application 'xgboost-breast-cancer-classifier' is ready at http://127.0.0.1:8000/.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ProxyActor pid=42374)\u001b[0m INFO 2025-04-09 14:27:31,923 proxy 127.0.0.1 -- Proxy starting on node 94fbfec9b89f75d90b7512b30d7076cf04fcad9cb384734bcd49a346 (HTTP port: 8000).\n",
      "\u001b[36m(ProxyActor pid=42374)\u001b[0m INFO 2025-04-09 14:27:31,942 proxy 127.0.0.1 -- Got updated endpoints: {}.\n",
      "\u001b[36m(ServeController pid=42385)\u001b[0m INFO 2025-04-09 14:27:31,964 controller 42385 -- Deploying new version of Deployment(name='XGBoostModel', app='xgboost-breast-cancer-classifier') (initial target replicas: 1).\n",
      "\u001b[36m(ProxyActor pid=42374)\u001b[0m INFO 2025-04-09 14:27:31,965 proxy 127.0.0.1 -- Got updated endpoints: {Deployment(name='XGBoostModel', app='xgboost-breast-cancer-classifier'): EndpointInfo(route='/', app_is_cross_language=False)}.\n",
      "\u001b[36m(ProxyActor pid=42374)\u001b[0m INFO 2025-04-09 14:27:31,969 proxy 127.0.0.1 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x10e8c0350>.\n",
      "\u001b[36m(ServeController pid=42385)\u001b[0m INFO 2025-04-09 14:27:32,066 controller 42385 -- Adding 1 replica to Deployment(name='XGBoostModel', app='xgboost-breast-cancer-classifier').\n",
      "\u001b[36m(ServeReplica:xgboost-breast-cancer-classifier:XGBoostModel pid=42381)\u001b[0m INFO 2025-04-09 14:27:44,082 xgboost-breast-cancer-classifier_XGBoostModel bwkqlb6a 0c8c5cc8-1b2a-4490-89e5-7b9a92e7e839 -- POST / 200 6.8ms\n",
      "\u001b[36m(ServeReplica:xgboost-breast-cancer-classifier:XGBoostModel pid=42381)\u001b[0m /Users/rdecal/new_src/ANYSCALE/new_tutorials/e2e-xgboost-rebase/.venv/lib/python3.12/site-packages/ray/serve/_private/replica.py:1173: UserWarning: Calling sync method 'pythonic_call' directly on the asyncio loop. In a future version, sync methods will be run in a threadpool by default. Ensure your sync methods are thread safe or keep the existing behavior by making them `async def`. Opt into the new behavior by setting RAY_SERVE_RUN_SYNC_IN_THREADPOOL=1.\n",
      "\u001b[36m(ServeReplica:xgboost-breast-cancer-classifier:XGBoostModel pid=42381)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(ServeReplica:xgboost-breast-cancer-classifier:XGBoostModel pid=42381)\u001b[0m INFO 2025-04-09 14:27:45,126 xgboost-breast-cancer-classifier_XGBoostModel bwkqlb6a 70d563cc-6be5-4f4b-a36d-82c5cc18d0e9 -- CALL pythonic_call OK 5.2ms\n"
     ]
    }
   ],
   "source": [
    "from ray.serve.handle import DeploymentHandle\n",
    "\n",
    "handle: DeploymentHandle = serve.run(xgboost_model, name=\"xgboost-breast-cancer-classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e2b045",
   "metadata": {},
   "source": [
    "We should see some logs indicating that the service is running locally:\n",
    "\n",
    "```bash\n",
    "INFO 2025-04-09 14:06:55,760 serve 31684 -- Started Serve in namespace \"serve\".\n",
    "INFO 2025-04-09 14:06:57,875 serve 31684 -- Application 'default' is ready at http://127.0.0.1:8000/.\n",
    "```\n",
    "\n",
    "We can also check whether it is running using [`serve.status()`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.status.html#ray.serve.status):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72794309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serve.status().applications[\"xgboost-breast-cancer-classifier\"].status == \"RUNNING\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060b974c",
   "metadata": {},
   "source": [
    "## Querying the Service\n",
    "\n",
    "### Using HTTP\n",
    "The most common way to query services is via an HTTP request. This invokes the `__call__` method we defined earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d99974c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 0.0434\n",
      "Ground truth: 0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://127.0.0.1:8000/\"\n",
    "response = requests.post(url, json=sample_input).json()\n",
    "\n",
    "print(f\"Prediction: {response['predictions'][0]:.4f}\")\n",
    "print(f\"Ground truth: {sample_target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff978c09",
   "metadata": {},
   "source": [
    "### Using Python\n",
    "\n",
    "For a more direct Pythonic way to query the model, you can use the deployment handle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96becd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-04-09 14:27:45,116 serve 42336 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x15f230110>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [0.043412428349256516]}\n"
     ]
    }
   ],
   "source": [
    "response = await handle.pythonic_call.remote(sample_input)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a358e256",
   "metadata": {},
   "source": [
    "This approach is useful if you need to interact with the service from a different process in the same Ray Cluster. If you need to regenerate the serve handle, you can use [`serve.get_deployment_handle`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.get_deployment_handle.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8408d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = serve.get_deployment_handle(\"XGBoostModel\", \"xgboost-breast-cancer-classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bff8bc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b>ðŸ”Ž Observability for Services</b>\n",
    "\n",
    "Observability for Ray Serve applications are automatically captured in the Ray dashboard and specifically the [Serve view](https://docs.ray.io/en/latest/ray-observability/getting-started.html#serve-view). Here we can view our service [deployments and their replicas](https://docs.ray.io/en/latest/serve/key-concepts.html#serve-key-concepts-deployment) and time-series metrics to see our service's health.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-xgboost/refs/heads/main/images/serve_dashboard.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f3b467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown service\n",
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792348c5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b>Anyscale Services</b>\n",
    "\n",
    "[Anyscale Services](https://docs.anyscale.com/platform/services/) ([API ref](https://docs.anyscale.com/reference/service-api/)) offers an extremely fault tolerant, scalable and optimized way to serve our Ray Serve applications.\n",
    "- we can [rollout and update](https://docs.anyscale.com/platform/services/update-a-service) our services with canary deployment (zero-downtime upgrades)\n",
    "- [monitor](https://docs.anyscale.com/platform/services/monitoring) our Services through a dedicated Service page, unified log viewer, tracing, set up alerts, etc.\n",
    "- scale a service (`num_replicas=auto`) and utilize replica compaction to consolidate nodes that are fractionally utilized\n",
    "- [head node fault tolerance](https://docs.anyscale.com/platform/services/production-best-practices#head-node-ft) (OSS Ray recovers from failed workers and replicas but not head node crashes)\n",
    "- serving [muliple applications](https://docs.anyscale.com/platform/services/multi-app) in a single Service\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-xgboost/refs/heads/main/images/canary.png\" width=1000>\n",
    "\n",
    "[RayTurbo Serve](https://docs.anyscale.com/rayturbo/rayturbo-serve) on Anyscale has even more functionality on top of Ray Serve:\n",
    "- **fast autoscaling and model loading** to get our services up and running even faster ([5x improvements](https://www.anyscale.com/blog/autoscale-large-ai-models-faster) even for LLMs)\n",
    "- 54% **higher QPS** and up-to 3x **streaming tokens per second** for high traffic serving use-cases (no proxy bottlenecks)\n",
    "- **replica compaction** into fewer nodes where possible to reduce resource fragmentation and improve hardware utilization\n",
    "- **zero-downtime** [incremental rollouts](https://docs.anyscale.com/platform/services/update-a-service/#resource-constrained-updates) so your service is never interrupted\n",
    "- [**different environments**](https://docs.anyscale.com/platform/services/multi-app/#multiple-applications-in-different-containers) for each service in a multi-serve application\n",
    "- **multi availability-zone** aware scheduling of Ray Serve replicas to provide higher redundancy to availability zone failures\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b0fdc4",
   "metadata": {},
   "source": [
    "**Note**: \n",
    "- we're using a `containerfile` to define our dependencies, but we could easily use a pre-built image as well.\n",
    "- we can specify the compute as a [compute config](https://docs.anyscale.com/configuration/compute-configuration/) or inline in a [Service config](https://docs.anyscale.com/reference/service-api/) file.\n",
    "- when we don't specify compute and when launching from a workspace, this defaults to the compute configuration of the Workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe314f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Production online service\n",
    "anyscale service deploy dist_xgboost.serve:xgboost_model --name=e2e-xgboost \\\n",
    "    --containerfile=\"/home/ray/default/containerfile\" \\\n",
    "    --working-dir=\"/home/ray/default\" \\\n",
    "    --exclude=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f40d122",
   "metadata": {},
   "source": [
    "Your service is now in production! The link to your endpoint should be visible in the logs. Here's an example of how you might query it:\n",
    "\n",
    "FIXME update\n",
    "\n",
    "```sh\n",
    "curl -X POST \"https://e2e-xgboost-bxauk.cld-kvedzwag2qa8i5bj.s.anyscaleuserdata.com/predict/\" \\\n",
    "     -H \"Authorization: Bearer <BEARER_TOKEN>\" \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\"url\": \"https://doggos-dataset.s3.us-west-2.amazonaws.com/samara.png\", \"k\": 4}'\n",
    "```\n",
    "\n",
    "To tear down the service, you can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59344bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Terminate service\n",
    "anyscale service terminate --name e2e-xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6a519d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b>CI/CD</b>\n",
    "\n",
    "While Anyscale [Jobs](https://docs.anyscale.com/platform/jobs/) and [Services](https://docs.anyscale.com/platform/services/) are great atomic concepts that help us productionize our workloads, they're also great for nodes in a larger ML DAG or [CI/CD workflow](https://docs.anyscale.com/ci-cd/). You can chain Jobs together, storge results and then serve your application with those artifacts. And from there, you can trigger updates to your service (and retrigger the Jobs) based on events, time, etc.  And while we can simply use the Anyscale CLI to integrate with any orchestration platform, Anyscale does support some purpose-built integrations ([Airflow](https://docs.anyscale.com/ci-cd/apache-airflow/), [Prefect](https://github.com/anyscale/prefect-anyscale)). \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-xgboost/refs/heads/main/images/cicd.png\" width=700>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
